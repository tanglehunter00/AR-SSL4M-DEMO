{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR-SSL4M Pretraining on Google Colab\n",
    "\n",
    "This notebook handles the setup and pretraining of the AR-SSL4M model using data from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset path\n",
    "import os\n",
    "dataset_path = '/content/drive/MyDrive/dataset/demo'\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Dataset found at {dataset_path}\")\n",
    "    print(os.listdir(dataset_path))\n",
    "else:\n",
    "    print(f\"Dataset NOT found at {dataset_path}. Please check your Drive structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Verification and Cleaning\n",
    "# This cell checks all .npy files in the dataset to ensure they are valid and not empty.\n",
    "# It will regenerate the 'colab_train_list.txt' excluding any corrupted files.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "drive_dataset_path = '/content/drive/MyDrive/dataset/demo'\n",
    "patch_dir = os.path.join(drive_dataset_path, 'patch_random_spatial')\n",
    "list_file_path = os.path.join(drive_dataset_path, 'colab_train_list.txt')\n",
    "\n",
    "valid_files = []\n",
    "corrupted_files = []\n",
    "\n",
    "if os.path.exists(patch_dir):\n",
    "    npy_files = [f for f in os.listdir(patch_dir) if f.endswith('.npy')]\n",
    "    print(f\"Checking {len(npy_files)} files in {patch_dir}...\")\n",
    "    \n",
    "    for f in tqdm(npy_files):\n",
    "        full_path = os.path.join(patch_dir, f)\n",
    "        try:\n",
    "            # Try loading the file\n",
    "            data = np.load(full_path, mmap_mode='r') # Use mmap_mode='r' for faster checking without full read\n",
    "            # Check shape/size if necessary, e.g.\n",
    "            if data.size == 0 or data.shape != (128, 128, 128):\n",
    "                 # Double check by fully loading if mmap is unsure or for strict size check\n",
    "                 data = np.load(full_path)\n",
    "                 if data.size == 0:\n",
    "                    print(f\"Skipping empty file: {f}\")\n",
    "                    corrupted_files.append(full_path)\n",
    "                    continue\n",
    "            \n",
    "            valid_files.append(full_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "            corrupted_files.append(full_path)\n",
    "\n",
    "    # Update the list file with only valid paths\n",
    "    with open(list_file_path, 'w') as f:\n",
    "        f.write('\\n'.join(valid_files))\n",
    "    \n",
    "    print(f\"\\nVerification complete.\")\n",
    "    print(f\"Valid files: {len(valid_files)}\")\n",
    "    print(f\"Corrupted/Empty files removed: {len(corrupted_files)}\")\n",
    "    print(f\"Updated training list at: {list_file_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Patch directory not found: {patch_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if not already present)\n",
    "# Cloning from your GitHub repository as requested\n",
    "!git clone https://github.com/tanglehunter00/AR-SSL4M-DEMO.git\n",
    "\n",
    "# IMPORTANT: If you are running this notebook and the code is NOT on Drive,\n",
    "# you need to upload the code files to Colab runtime.\n",
    "\n",
    "project_root = '/content/AR-SSL4M-DEMO' \n",
    "import os\n",
    "if os.path.exists(project_root):\n",
    "    %cd {project_root}\n",
    "else:\n",
    "    print(\"Project root not found. Please clone or upload your code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install timm monai transformers fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataset configuration paths dynamically\n",
    "# We need to point the dataset config to the list files in Google Drive\n",
    "\n",
    "# Assuming your list files are also in the dataset folder on Drive\n",
    "# You might need to generate these list files if they contain absolute local paths from your PC.\n",
    "# Here we create a new list file based on the Drive path.\n",
    "\n",
    "import os\n",
    "\n",
    "drive_dataset_path = '/content/drive/MyDrive/dataset/demo'\n",
    "patch_dir = os.path.join(drive_dataset_path, 'patch_random_spatial')\n",
    "list_file_path = os.path.join(drive_dataset_path, 'colab_train_list.txt')\n",
    "\n",
    "if os.path.exists(patch_dir):\n",
    "    npy_files = [f for f in os.listdir(patch_dir) if f.endswith('.npy')]\n",
    "    full_paths = [os.path.join(patch_dir, f) for f in npy_files]\n",
    "    \n",
    "    with open(list_file_path, 'w') as f:\n",
    "        f.write('\\n'.join(full_paths))\n",
    "    print(f\"Created training list at {list_file_path} with {len(full_paths)} files.\")\n",
    "else:\n",
    "    print(\"Patch directory not found. Please ensure 'patch_random_spatial' exists inside 'dataset/demo'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify configs/datasets.py to use the Colab path\n",
    "# We will do this by writing a temporary config file or modifying the file in place if possible.\n",
    "# A safer way is to rely on the fact that we can update the config dynamically, \n",
    "# but the current codebase reads from a file. \n",
    "# Let's modify pretrain/configs/datasets.py directly.\n",
    "\n",
    "config_path = 'pretrain/configs/datasets.py'\n",
    "\n",
    "new_config_content = f\"\"\"\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class custom_dataset:\n",
    "    dataset: str = \"custom_dataset\"\n",
    "    file: str = \"image_dataset.py\"\n",
    "    train_split: str = \"train\"\n",
    "    test_split: str = \"validation\"\n",
    "    # Pointing to the generated list file on Drive\n",
    "    spatial_path: str = \"{list_file_path}\"\n",
    "    contrast_path: str = \"{list_file_path}\"\n",
    "    semantic_path: str = \"{list_file_path}\"\n",
    "    img_size = [128, 128, 128]\n",
    "    patch_size = [16, 16, 16]\n",
    "    attention_type = 'prefix'\n",
    "    add_series_data = False\n",
    "    add_spatial_data = True\n",
    "    is_subset = False\n",
    "    series_length = 4\n",
    "\"\"\"\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(new_config_content)\n",
    "\n",
    "print(\"Updated datasets.py configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Pretraining\n",
    "# Set batch size to 32 as requested\n",
    "# Using newModel.py (ensure you imported ReconModel from newModel in main.py if that was the intent, \n",
    "# OR ensure model.py is updated with your changes)\n",
    "\n",
    "# Note: The user requested to use 'newModel.py'. \n",
    "# You might need to rename newModel.py to model.py OR modify main.py to import from newModel.\n",
    "# Here we assume main.py still imports from model.py. \n",
    "# We will rename newModel.py to model.py for this run to ensure the new logic is used.\n",
    "\n",
    "!cp pretrain/newModel.py pretrain/model.py\n",
    "\n",
    "# Switch to pretrain directory\n",
    "%cd pretrain\n",
    "\n",
    "!mkdir -p /content/drive/MyDrive/dataset/demo/output \n",
    "\n",
    "# Run training\n",
    "!python main.py \\\n",
    "    --enable_fsdp False \\\n",
    "    --output_dir /content/drive/MyDrive/dataset/demo/output \\\n",
    "    --batch_size_training 16 \\\n",
    "    --num_epochs 30 \\\n",
    "    --save_metrics True \\\n",
    "    --num_workers_dataloader 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
