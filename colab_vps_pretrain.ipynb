{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUXbsStuYd6i"
      },
      "source": [
        "# AR-SSL4M Pretraining on Google Colab Using VPS and local drive\n",
        "\n",
        "This notebook handles the setup and pretraining of the AR-SSL4M model using data from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlqspGakYd6j",
        "outputId": "a4c7aad2-597e-485e-e2fd-707e66506240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. å®‰è£…æ ¸å¿ƒç½‘ç»œç»„ä»¶ & æ·±åº¦å­¦ä¹ ä¾èµ–\n",
        "# ==========================================\n",
        "print(\"--- æ­£åœ¨å®‰è£… Tailscale & S3 è®¤è¯ç»„ä»¶ ---\")\n",
        "\n",
        "# å®‰è£… Tailscale å®˜æ–¹å®¢æˆ·ç«¯\n",
        "!curl -fsSL https://tailscale.com/install.sh | sh\n",
        "\n",
        "# å®‰è£… Python ç½‘ç»œè¯·æ±‚ã€S3 åè®®åŠ SOCKS ä»£ç†æ”¯æŒ\n",
        "!pip install boto3 botocore PySocks requests -q\n",
        "\n",
        "# å®‰è£… AR-SSL4M æ¨¡å‹è¿è¡Œæ‰€éœ€çš„ä¾èµ–åº“\n",
        "!pip install timm monai transformers fire -q\n",
        "\n",
        "# ==========================================\n",
        "# 2. æ¸…ç†ç¯å¢ƒå¹¶å¯åŠ¨ç½‘ç»œå †æ ˆ\n",
        "# ==========================================\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"\\n--- æ­£åœ¨é‡ç½® Tailscale ç½‘ç»œç¯å¢ƒ ---\")\n",
        "# æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ®‹ç•™è¿›ç¨‹\n",
        "!pkill -9 tailscaled 2>/dev/null\n",
        "!rm -rf /var/run/tailscale/tailscaled.pid\n",
        "\n",
        "# å¯åŠ¨ Tailscale å®ˆæŠ¤è¿›ç¨‹å¹¶å¼€å¯ SOCKS5 ä»£ç†å…¥å£ (1055 ç«¯å£)\n",
        "!nohup tailscaled --tun=userspace-networking --socks5-server=localhost:1055 --state=/var/lib/tailscale/tailscaled.state > /tmp/tailscale.log 2>&1 &\n",
        "time.sleep(5)\n",
        "\n",
        "# ==========================================\n",
        "# 3. éš§é“è®¤è¯ (è¯·ç‚¹å‡»ç”Ÿæˆçš„é“¾æ¥)\n",
        "# ==========================================\n",
        "print(\"\\n--- è¯·ç‚¹å‡»ä¸‹æ–¹é“¾æ¥è¿›è¡Œ Tailscale è®¤è¯ ---\")\n",
        "# æ‰§è¡Œè®¤è¯ä¸Šçº¿\n",
        "!tailscale up --accept-dns=false\n",
        "\n",
        "print(\"\\nâœ… ç¯å¢ƒå‡†å¤‡å°±ç»ªï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnzMPNSlYd6j",
        "outputId": "8a0db91b-75d5-4a97-8ef1-0abd0e6d4077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['LIDC-IDRI', 'AR-SSL4M-DEMO', 'patch_random_spatial', 'Untitled folder', 'colab_train_list.txt', 'output']\n",
            "Dataset found at /content/drive/MyDrive/dataset/LIDC-IDRI\n",
            "['LIDC-IDRI', 'AR-SSL4M-DEMO', 'patch_random_spatial', 'Untitled folder', 'colab_train_list.txt', 'output']\n"
          ]
        }
      ],
      "source": [
        "# éªŒè¯é¢„ç­¾å URL åˆ—è¡¨æ–‡ä»¶ (ä¸å†æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¤¹ï¼Œæ”¹ä¸ºæ£€æŸ¥ URL æ¸…å•)\n",
        "import os\n",
        "\n",
        "# è¿™æ˜¯ä½ å­˜æ”¾åœ¨ Google Drive é‡Œçš„å…¨é‡ URL åˆ—è¡¨è·¯å¾„\n",
        "urls_list_path = '/content/drive/MyDrive/dataset/pretrain/DeepLesion/urls.txt'\n",
        "\n",
        "if os.path.exists(urls_list_path):\n",
        "    print(f\"âœ… æ‰¾åˆ° URL åˆ—è¡¨æ–‡ä»¶: {urls_list_path}\")\n",
        "    \n",
        "    with open(urls_list_path, 'r') as f:\n",
        "        all_lines = f.readlines()\n",
        "        \n",
        "    # è¿‡æ»¤æ‰å…ƒæ•°æ®ï¼Œåªç»Ÿè®¡çœŸæ­£çš„æ•°æ®é“¾æ¥\n",
        "    valid_urls = [line.strip() for line in all_lines if '.npy?' in line and '.minio.sys' not in line]\n",
        "    \n",
        "    print(f\"ğŸ“Š åˆ—è¡¨éªŒè¯ç»“æœ:\")\n",
        "    print(f\"   - æ€»è¡Œæ•°: {len(all_lines)}\")\n",
        "    print(f\"   - æœ‰æ•ˆ .npy é“¾æ¥æ•°: {len(valid_urls)}\")\n",
        "    \n",
        "    if len(valid_urls) > 0:\n",
        "        print(f\"ğŸ”— æ ·æœ¬é¢„è§ˆ: {valid_urls[0][:100]}...\") # åªæ‰“å°å‰100å­—ç¬¦é˜²æ­¢åˆ·å±\n",
        "    else:\n",
        "        print(\"âŒ è­¦å‘Šï¼šåˆ—è¡¨ä¸­æœªå‘ç°æœ‰æ•ˆçš„ .npy é“¾æ¥ï¼è¯·æ£€æŸ¥ Windows ç«¯çš„ç”Ÿæˆé€»è¾‘ã€‚\")\n",
        "else:\n",
        "    print(f\"âŒ é”™è¯¯ï¼šåœ¨ {urls_list_path} æœªæ‰¾åˆ°æ–‡ä»¶ã€‚\")\n",
        "    print(\"è¯·ç¡®è®¤ä½ å·²å°† Windows ç”Ÿæˆçš„ urls.txt ä¸Šä¼ è‡³è¯¥ Drive è·¯å¾„ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAXGyrJQYd6j",
        "outputId": "7108ef56-4a57-4a77-c4e5-6d8b12664912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking 24850 files in /content/drive/MyDrive/dataset/LIDC-IDRI/patch_random_spatial...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24850/24850 [4:35:15<00:00,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification complete.\n",
            "Valid files: 24850\n",
            "Corrupted/Empty files removed: 0\n",
            "Updated training list at: /content/drive/MyDrive/dataset/LIDC-IDRI/colab_train_list.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Verification (Tailscale Online Mode)\n",
        "# ä¸å†å…¨å±€æ ¡éªŒæ–‡ä»¶ï¼Œæ”¹ä¸ºéšæœºæŠ½æ ·æµ‹è¯•éš§é“é“¾è·¯å’Œæ•°æ®æ ¼å¼\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import io\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "urls_list_path = '/content/drive/MyDrive/dataset/pretrain/DeepLesion/urls.txt'\n",
        "sample_size = 10  # æŠ½æ ·æµ‹è¯• 10 ä¸ªæ–‡ä»¶å³å¯ï¼Œæ— éœ€å…¨éƒ¨æ ¡éªŒ\n",
        "\n",
        "# ä»£ç†é…ç½®\n",
        "proxies = {\n",
        "    'http': 'socks5h://127.0.0.1:1055',\n",
        "    'https': 'socks5h://127.0.0.1:1055'\n",
        "}\n",
        "\n",
        "if os.path.exists(urls_list_path):\n",
        "    with open(urls_list_path, 'r') as f:\n",
        "        urls = [line.strip() for line in f if '.npy?' in line and '.minio.sys' not in line]\n",
        "    \n",
        "    print(f\"--- æ­£åœ¨å¯¹ {len(urls)} ä¸ªè¿œç¨‹æ–‡ä»¶è¿›è¡ŒéšæœºæŠ½æ ·æ ¡éªŒ ({sample_size}ä¸ª) ---\")\n",
        "    \n",
        "    # éšæœºæŠ½æ ·\n",
        "    test_samples = random.sample(urls, min(sample_size, len(urls)))\n",
        "    \n",
        "    success_count = 0\n",
        "    for url in tqdm(test_samples):\n",
        "        try:\n",
        "            # é€šè¿‡éš§é“æ‹‰å–æ•°æ®åˆ°å†…å­˜\n",
        "            r = requests.get(url, proxies=proxies, timeout=15)\n",
        "            r.raise_for_status()\n",
        "            \n",
        "            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ numpy æ•°æ®\n",
        "            data = np.load(io.BytesIO(r.content))\n",
        "            \n",
        "            # éªŒè¯ç»´åº¦ (å‚è€ƒåŸä»£ç  128x128x128)\n",
        "            if data.shape == (128, 128, 128):\n",
        "                success_count += 1\n",
        "            else:\n",
        "                print(f\"\\nâš ï¸ ç»´åº¦ä¸åŒ¹é…: {data.shape}, URL: {url[:50]}...\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ æ ¡éªŒå¤±è´¥: {e}\")\n",
        "\n",
        "    print(f\"\\n--- æ ¡éªŒå®Œæˆ ---\")\n",
        "    print(f\"âœ… æˆåŠŸé€šè¿‡æµ‹è¯•: {success_count}/{len(test_samples)}\")\n",
        "    \n",
        "    if success_count == len(test_samples):\n",
        "        print(\"ğŸš€ éš§é“é“¾è·¯ä¸æ•°æ®æ ¼å¼å®Œå…¨æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒã€‚\")\n",
        "    else:\n",
        "        print(\"ğŸ›‘ é“¾è·¯ä¸ç¨³å®šæˆ–æ•°æ®æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥ Tailscale çŠ¶æ€ã€‚\")\n",
        "else:\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ°åˆ—è¡¨æ–‡ä»¶: {urls_list_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtnXgthqYd6j",
        "outputId": "e823d0df-7f88-4d5d-f03a-93d6a0db2b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AR-SSL4M-DEMO'...\n",
            "remote: Enumerating objects: 290, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 290 (delta 14), reused 9 (delta 9), pack-reused 274 (from 1)\u001b[K\n",
            "Receiving objects: 100% (290/290), 1.39 MiB | 27.36 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n",
            "/content/AR-SSL4M-DEMO\n"
          ]
        }
      ],
      "source": [
        "# 1. å…‹éš†ä»£ç ä»“åº“å¹¶è¿›å…¥å·¥ä½œç›®å½•\n",
        "project_root = '/content/AR-SSL4M-DEMO'\n",
        "\n",
        "# å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤ï¼ˆç¡®ä¿æ¯æ¬¡éƒ½æ˜¯ä» GitHub æ‹‰å–æœ€æ–°ä»£ç ï¼‰\n",
        "if os.path.exists(project_root):\n",
        "    import shutil\n",
        "    shutil.rmtree(project_root)\n",
        "\n",
        "# å…‹éš†ä½ çš„ä»“åº“\n",
        "!git clone https://github.com/tanglehunter00/AR-SSL4M-DEMO.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(project_root):\n",
        "    # åˆ‡æ¢å·¥ä½œç›®å½•åˆ° pretrain æ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿åç»­è¿è¡Œ main.py\n",
        "    %cd {project_root}/pretrain\n",
        "    print(f\"âœ… å·²è¿›å…¥é¡¹ç›®æ ¹ç›®å½•: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"âŒ é”™è¯¯ï¼šä»“åº“å…‹éš†å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– GitHub é“¾æ¥ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRdRFsfUYd6j",
        "outputId": "aa4a4f6c-820f-4b3e-96d9-04ec33c6f307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Collecting monai\n",
            "  Downloading monai-1.5.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Collecting fire\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire) (3.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Downloading monai-1.5.1-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, monai\n",
            "Successfully installed fire-0.7.1 monai-1.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install timm monai transformers fire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5oZC_lCYd6j",
        "outputId": "3569b471-f7bb-47c8-c5d5-a77b048fcb2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created training list at /content/drive/MyDrive/dataset/LIDC-IDRI/colab_train_list.txt with 24850 files.\n"
          ]
        }
      ],
      "source": [
        "# Update dataset configuration to use Online URLs\n",
        "# ä¸å†æ‰«æ Drive æ–‡ä»¶å¤¹ï¼Œç›´æ¥å°† urls.txt ä½œä¸ºæ•°æ®æºåˆ—è¡¨\n",
        "\n",
        "import os\n",
        "\n",
        "# 1. å®šä¹‰ URL åˆ—è¡¨çš„æ¥æºï¼ˆæ¥è‡ªä½ çš„ Google Driveï¼‰\n",
        "urls_list_source = '/content/drive/MyDrive/dataset/pretrain/DeepLesion/urls.txt'\n",
        "\n",
        "# 2. å®šä¹‰è®­ç»ƒæ—¶ä½¿ç”¨çš„åˆ—è¡¨è·¯å¾„ï¼ˆä¿æŒä¸åŸä»£ç å˜é‡åä¸€è‡´ï¼Œé˜²æ­¢ main.py æŠ¥é”™ï¼‰\n",
        "list_file_path = urls_list_source\n",
        "\n",
<<<<<<< HEAD
        "if os.path.exists(urls_list_source):\n",
        "    # è¯»å–å¹¶ç®€å•éªŒè¯åˆ—è¡¨è¡Œæ•°\n",
        "    with open(urls_list_source, 'r') as f:\n",
        "        url_count = len([line for line in f if '.npy?' in line])\n",
        "    \n",
        "    print(f\"âœ… æ•°æ®é…ç½®å·²æ›´æ–°ï¼\")\n",
        "    print(f\"ğŸ”— è®­ç»ƒåˆ—è¡¨æŒ‡å‘: {list_file_path}\")\n",
        "    print(f\"ğŸ“„ æ£€æµ‹åˆ°æœ‰æ•ˆæ•°æ®é“¾æ¥: {url_count} æ¡\")\n",
        "    print(\"ğŸ’¡ æç¤ºï¼šç³»ç»Ÿå°†é€šè¿‡è¿™äº›é¢„ç­¾å URL ç›´æ¥ä»ä½ çš„ PC æŠ“å–æ•°æ®ï¼Œä¸å†å ç”¨ Colab ç¡¬ç›˜ã€‚\")\n",
=======
        "if os.path.exists(local_mount_path):\n",
        "    # è·å–æŒ‚è½½ç›®å½•ä¸‹æ‰€æœ‰çš„ .npy æ–‡ä»¶\n",
        "    npy_files = [f for f in os.listdir(local_mount_path) if f.endswith('.npy')]\n",
        "\n",
        "    # ç”Ÿæˆç»å¯¹è·¯å¾„åˆ—è¡¨\n",
        "    full_paths = [os.path.join(local_mount_path, f) for f in npy_files]\n",
        "\n",
        "    # å°†è·¯å¾„å†™å…¥ txt æ–‡ä»¶\n",
        "    with open(list_file_path, 'w') as f:\n",
        "        f.write('\\n'.join(full_paths))\n",
        "\n",
        "    print(f\"âœ… æˆåŠŸï¼šå·²åˆ›å»ºè®­ç»ƒåˆ—è¡¨ {list_file_path}\")\n",
        "    print(f\"ğŸ“„ æ ·æœ¬æ•°é‡: {len(full_paths)} ä¸ªæ–‡ä»¶\")\n",
        "    if len(full_paths) > 0:\n",
        "        print(f\"ğŸ”— ç¬¬ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ç¤ºä¾‹: {full_paths[0]}\")\n",
>>>>>>> 8d245c2be78129b163f0eb3d32b88e51c87e1afb
        "else:\n",
        "    print(f\"âŒ é”™è¯¯ï¼šåœ¨ {urls_list_source} æœªæ‰¾åˆ° URL åˆ—è¡¨ã€‚\")\n",
        "    print(\"è¯·ç¡®è®¤ä½ å·²å°† Windows ç”Ÿæˆçš„ urls.txt ä¸Šä¼ è‡³è¯¥ Drive è·¯å¾„ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf8E-fLkYd6j",
        "outputId": "66136d27-2731-4fdf-a10a-4b3265fd8d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated datasets.py configuration.\n"
          ]
        }
      ],
      "source": [
        "# ä¿®æ”¹ datasets.py é…ç½®æ–‡ä»¶ï¼Œå°†æ•°æ®è·¯å¾„æŒ‡å‘ URL åˆ—è¡¨\n",
        "# æ³¨æ„ï¼šæ­¤æ—¶æˆ‘ä»¬å·²ç»åœ¨ /content/AR-SSL4M-DEMO/pretrain ç›®å½•ä¸‹\n",
        "\n",
        "import os\n",
        "\n",
        "# ä¿®æ­£ç›¸å¯¹è·¯å¾„ï¼šå› ä¸ºå½“å‰å·²åœ¨ pretrain æ–‡ä»¶å¤¹å†…\n",
        "config_path = 'configs/datasets.py'\n",
        "\n",
        "# æ³¨å…¥é…ç½®ä¿¡æ¯\n",
        "new_config_content = f\"\"\"\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class custom_dataset:\n",
        "    dataset: str = \"custom_dataset\"\n",
        "    file: str = \"image_dataset.py\"\n",
        "    train_split: str = \"train\"\n",
        "    test_split: str = \"validation\"\n",
        "    # è¿™é‡Œ list_file_path æŒ‡å‘çš„æ˜¯åŒ…å«é¢„ç­¾å URL çš„ urls.txt\n",
        "    spatial_path: str = \"{list_file_path}\"\n",
        "    contrast_path: str = \"{list_file_path}\"\n",
        "    semantic_path: str = \"{list_file_path}\"\n",
        "    img_size = [128, 128, 128]\n",
        "    patch_size = [16, 16, 16]\n",
        "    attention_type = 'prefix'\n",
        "    add_series_data = False\n",
        "    add_spatial_data = True\n",
        "    is_subset = False\n",
        "    series_length = 4\n",
        "\"\"\"\n",
        "\n",
        "# æ‰§è¡Œå†™å…¥\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(new_config_content)\n",
        "\n",
        "print(f\"âœ… æˆåŠŸæ›´æ–° {config_path}\")\n",
        "print(f\"ğŸš€ è®­ç»ƒå°†ä»ä»¥ä¸‹ URL åˆ—è¡¨æ‹‰å–æ•°æ®: {list_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ino-Du0PYd6k",
        "outputId": "0c5e7bcd-9b69-49db-ee3a-fc610c3d0c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'pretrain/newModel.py': No such file or directory\n",
            "[Errno 2] No such file or directory: 'pretrain'\n",
            "/content/AR-SSL4M-DEMO/pretrain\n",
            "2026-01-26 21:45:56.481968: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-26 21:45:56.500074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769463956.522029    4891 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769463956.528501    4891 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769463956.545032    4891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769463956.545074    4891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769463956.545077    4891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769463956.545080    4891 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-26 21:45:56.550162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/AR-SSL4M-DEMO/pretrain/utils/model_checkpointing_utils.py:15: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
            "  from torch.distributed._shard.checkpoint import (\n",
            "--> Model has 91.327744 Million params\n",
            "\n",
            "--> Training Set Length = 24800\n",
            "--> Validation Set Length = 50\n",
            "Training Epoch: 1/5, step 192/193 completed (loss: 0.9311574101448059): 100% 193/193 [1:33:52<00:00, 29.19s/it]\n",
            "Max CUDA memory allocated was 66 GB\n",
            "Max CUDA memory reserved was 71 GB\n",
            "Peak active CUDA memory was 66 GB\n",
            "Cuda Malloc retires : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 4 GB\n",
            "evaluating Epoch: 100% 50/50 [00:15<00:00,  3.14it/s]\n",
            " eval_epoch_loss=tensor(0.9108, device='cuda:0')\n",
            "--> saving model ...\n",
            "model checkpoint saved for epoch 0 at /content/drive/MyDrive/dataset/demo/output/checkpoints/0/0.pth\n",
            "\n",
            "best eval loss on epoch 1 is 0.9107836484909058\n",
            "Epoch 1: train_epoch_loss=0.9743, epoch time 5633.976602776s\n",
            "Training Epoch: 2/5, step 192/193 completed (loss: 0.879838764667511): 100% 193/193 [59:28<00:00, 18.49s/it]\n",
            "Max CUDA memory allocated was 66 GB\n",
            "Max CUDA memory reserved was 73 GB\n",
            "Peak active CUDA memory was 66 GB\n",
            "Cuda Malloc retires : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 5 GB\n",
            "evaluating Epoch: 100% 50/50 [00:07<00:00,  6.80it/s]\n",
            " eval_epoch_loss=tensor(0.8876, device='cuda:0')\n",
            "--> saving model ...\n",
            "model checkpoint saved for epoch 1 at /content/drive/MyDrive/dataset/demo/output/checkpoints/1/1.pth\n",
            "\n",
            "best eval loss on epoch 2 is 0.8875880837440491\n",
            "Epoch 2: train_epoch_loss=0.8940, epoch time 3569.7441964160007s\n",
            "Training Epoch: 3/5, step 192/193 completed (loss: 0.8108476996421814): 100% 193/193 [56:26<00:00, 17.55s/it]\n",
            "Max CUDA memory allocated was 66 GB\n",
            "Max CUDA memory reserved was 72 GB\n",
            "Peak active CUDA memory was 66 GB\n",
            "Cuda Malloc retires : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 5 GB\n",
            "evaluating Epoch: 100% 50/50 [00:07<00:00,  6.73it/s]\n",
            " eval_epoch_loss=tensor(0.8116, device='cuda:0')\n",
            "--> saving model ...\n",
            "model checkpoint saved for epoch 2 at /content/drive/MyDrive/dataset/demo/output/checkpoints/2/2.pth\n",
            "\n",
            "best eval loss on epoch 3 is 0.8115578293800354\n",
            "Epoch 3: train_epoch_loss=0.8456, epoch time 3387.4065633749997s\n",
            "Training Epoch: 4/5, step 192/193 completed (loss: 0.7827053666114807): 100% 193/193 [56:38<00:00, 17.61s/it]\n",
            "Max CUDA memory allocated was 67 GB\n",
            "Max CUDA memory reserved was 76 GB\n",
            "Peak active CUDA memory was 67 GB\n",
            "Cuda Malloc retires : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 5 GB\n",
            "evaluating Epoch: 100% 50/50 [00:07<00:00,  6.50it/s]\n",
            " eval_epoch_loss=tensor(0.7700, device='cuda:0')\n",
            "--> saving model ...\n",
            "model checkpoint saved for epoch 3 at /content/drive/MyDrive/dataset/demo/output/checkpoints/3/3.pth\n",
            "\n",
            "best eval loss on epoch 4 is 0.769963800907135\n",
            "Epoch 4: train_epoch_loss=0.7948, epoch time 3399.5032498810015s\n",
            "Training Epoch: 5/5, step 192/193 completed (loss: 0.7769752144813538): 100% 193/193 [1:03:30<00:00, 19.74s/it]\n",
            "Max CUDA memory allocated was 66 GB\n",
            "Max CUDA memory reserved was 76 GB\n",
            "Peak active CUDA memory was 66 GB\n",
            "Cuda Malloc retires : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 5 GB\n",
            "evaluating Epoch: 100% 50/50 [00:07<00:00,  7.12it/s]\n",
            " eval_epoch_loss=tensor(0.7799, device='cuda:0')\n",
            "Epoch 5: train_epoch_loss=0.7803, epoch time 3811.6671024720017s\n",
            "\n",
            "Training completed! All epoch train losses: [0.9743292331695557, 0.8939589262008667, 0.8456056118011475, 0.794790506362915, 0.7803228497505188]\n",
            "Key: avg_train_loss, Value: 0.8578014254570008\n",
            "Key: avg_eval_loss, Value: 0.829971432685852\n",
            "Key: avg_epoch_time, Value: 3960.459542984\n",
            "Key: avg_checkpoint_time, Value: 7.3147552146003365\n",
            "Key: metrics_filename, Value: /content/drive/MyDrive/dataset/demo/output/metrics_data_None-2026-01-26_21-46-05.json\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# æœ€ç»ˆæ­¥éª¤ï¼šå¯åŠ¨åœ¨çº¿é¢„è®­ç»ƒ\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "\n",
        "# 1. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ (ç”¨äºä¿å­˜æƒé‡å’Œ Metrics)\n",
        "output_dir = \"/content/drive/MyDrive/dataset/LIDC-IDRI/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"--- ğŸš€ æ­£åœ¨å¯åŠ¨ AR-SSL4M åœ¨çº¿é¢„è®­ç»ƒ ---\")\n",
        "print(f\"æ•°æ®æº: {list_file_path}\")\n",
        "print(f\"è¾“å‡ºè·¯å¾„: {output_dir}\")\n",
        "\n",
        "# 2. è¿è¡Œè®­ç»ƒ (é’ˆå¯¹åœ¨çº¿æ¨¡å¼ä¼˜åŒ–å‚æ•°)\n",
        "# --batch_size_training: å»ºè®®æ ¹æ® A100 æ˜¾å­˜å’Œç½‘ç»œå¸¦å®½è®¾ä¸º 16 æˆ– 32ï¼ŒåŸä»£ç  128 å¯¹ä¸­ç»§å‹åŠ›å¤ªå¤§\n",
        "# --num_workers_dataloader: å¿…é¡»è°ƒä½ï¼è®¾ä¸º 2 å¯åœ¨ä¸‹è½½å¹¶å‘ä¸ç¨³å®šæ€§ä¹‹é—´å–å¾—å¹³è¡¡\n",
        "# --enable_fsdp: è®¾ä¸º Falseï¼Œå•å¡ A100 æ— éœ€åˆ†ç‰‡\n",
        "\n",
        "!python main.py \\\n",
        "    --enable_fsdp False \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --batch_size_training 32 \\\n",
        "    --num_epochs 5 \\\n",
        "    --save_metrics True \\\n",
        "    --num_workers_dataloader 2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}