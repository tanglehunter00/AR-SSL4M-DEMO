{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUXbsStuYd6i"
      },
      "source": [
        "# AR-SSL4M Pretraining on Google Colab\n",
        "\n",
        "This notebook handles the setup and pretraining of the AR-SSL4M model using data from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlqspGakYd6j",
        "outputId": "73f3d46a-7d8b-44ce-e436-dff631f82560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnzMPNSlYd6j",
        "outputId": "2bd40f4a-7b8c-490a-a981-81cb854c001c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['LIDC-IDRI', 'AR-SSL4M-DEMO', 'patch_random_spatial', 'Untitled folder', 'output', 'pretrain_lists', 'colab_train_list.txt']\n",
            "Dataset found at /content/drive/MyDrive/dataset/LIDC-IDRI\n",
            "['LIDC-IDRI', 'AR-SSL4M-DEMO', 'patch_random_spatial', 'Untitled folder', 'output', 'pretrain_lists', 'colab_train_list.txt']\n"
          ]
        }
      ],
      "source": [
        "# Check dataset path\n",
        "import os\n",
        "dataset_path = '/content/drive/MyDrive/dataset/LIDC-IDRI'\n",
        "print(os.listdir('/content/drive/MyDrive/dataset/LIDC-IDRI'))\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset found at {dataset_path}\")\n",
        "    print(os.listdir(dataset_path))\n",
        "else:\n",
        "    print(f\"Dataset NOT found at {dataset_path}. Please check your Drive structure.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAXGyrJQYd6j",
        "outputId": "7108ef56-4a57-4a77-c4e5-6d8b12664912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking 24850 files in /content/drive/MyDrive/dataset/LIDC-IDRI/patch_random_spatial...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24850/24850 [4:35:15<00:00,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification complete.\n",
            "Valid files: 24850\n",
            "Corrupted/Empty files removed: 0\n",
            "Updated training list at: /content/drive/MyDrive/dataset/LIDC-IDRI/colab_train_list.txt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Verification and Cleaning (Spatial / LIDC)\n",
        "# Checks .npy files in patch_random_spatial and patch_random_lidc.\n",
        "# Regenerates list excluding corrupted files. Run list generation (next cell) after this.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "drive_dataset_path = '/content/drive/MyDrive/dataset/LIDC-IDRI'\n",
        "list_dir = os.path.join(drive_dataset_path, 'pretrain_lists')\n",
        "os.makedirs(list_dir, exist_ok=True)\n",
        "\n",
        "# Check both spatial dirs\n",
        "patch_dirs_to_check = [\n",
        "    os.path.join(drive_dataset_path, 'patch_random_spatial'),\n",
        "    os.path.join(drive_dataset_path, 'AR-SSL4M-DEMO', 'pretrain', 'data', 'patch_random_lidc'),\n",
        "]\n",
        "valid_files = []\n",
        "corrupted_files = []\n",
        "\n",
        "for patch_dir in patch_dirs_to_check:\n",
        "    if not os.path.exists(patch_dir):\n",
        "        print(f\"Skipping (not found): {patch_dir}\")\n",
        "        continue\n",
        "    npy_files = [f for f in os.listdir(patch_dir) if f.endswith('.npy')]\n",
        "    print(f\"Checking {len(npy_files)} files in {patch_dir}...\")\n",
        "    for f in tqdm(npy_files):\n",
        "        full_path = os.path.join(patch_dir, f)\n",
        "        try:\n",
        "            data = np.load(full_path, mmap_mode='r')\n",
        "            if data.size == 0 or data.shape != (128, 128, 128):\n",
        "                data = np.load(full_path)\n",
        "                if data.size == 0:\n",
        "                    corrupted_files.append(full_path)\n",
        "                    continue\n",
        "            valid_files.append(full_path)\n",
        "        except Exception as e:\n",
        "            corrupted_files.append(full_path)\n",
        "\n",
        "spatial_list_path = os.path.join(list_dir, 'train_spatial.txt')\n",
        "with open(spatial_list_path, 'w') as f:\n",
        "    f.write('\\n'.join(valid_files))\n",
        "print(f\"\\nVerification complete. Valid: {len(valid_files)}, Corrupted: {len(corrupted_files)}\")\n",
        "print(f\"Spatial list saved to: {spatial_list_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XquxYzGejIUW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "\n",
        "# 如果断连了，取消下面这行的注释重新挂载\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "drive_dataset_path = '/content/drive/MyDrive/dataset'\n",
        "tar_root = os.path.join(drive_dataset_path, 'pretrain', 'BraTS23_Data', 'tar_data')\n",
        "list_dir = os.path.join(drive_dataset_path, 'LIDC-IDRI', 'pretrain_lists')\n",
        "os.makedirs(list_dir, exist_ok=True)\n",
        "contrast_list_path = os.path.join(list_dir, 'train_contrast.txt')\n",
        "\n",
        "# --- 1. 读取已经处理过的 tar 包，实现断点续传 ---\n",
        "processed_tars = set()\n",
        "if os.path.exists(contrast_list_path):\n",
        "    with open(contrast_list_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if ':' in line:\n",
        "                # 提取 tar_path: tar_path:base\n",
        "                processed_tars.add(line.split(':')[0])\n",
        "print(f\"Skipping {len(processed_tars)} already processed tar files (Resume mode).\")\n",
        "\n",
        "if os.path.exists(tar_root):\n",
        "    all_tars = [os.path.join(r, f) for r, _, files in os.walk(tar_root) for f in files if f.endswith('.tar.gz')]\n",
        "    total_tars = len(all_tars)\n",
        "\n",
        "    # --- 2. 使用 'a' (append) 模式打开文件，实时写入 ---\n",
        "    with open(contrast_list_path, 'a') as out_f:\n",
        "        for i, tar_path in enumerate(all_tars, 1):\n",
        "            file_name = os.path.basename(tar_path)\n",
        "\n",
        "            if tar_path in processed_tars:\n",
        "                print(f\"[{i}/{total_tars}] Skipping: {file_name}\") # 不想刷屏跳过信息可以注释掉\n",
        "                continue\n",
        "\n",
        "            print(f\"[{i}/{total_tars}] >>> START: {file_name}\")\n",
        "            try:\n",
        "                print(f\"  [Step 1/4] Opening...\")\n",
        "                with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                    print(f\"  [Step 2/4] Reading names...\")\n",
        "                    names = tar.getnames()\n",
        "                    names_set = set(names)\n",
        "\n",
        "                    print(f\"  [Step 3/4] Matching...\")\n",
        "                    found_count = 0\n",
        "                    for n in names:\n",
        "                        if n.endswith('.t1n.npy'):\n",
        "                            base = n[:-len('.t1n.npy')]\n",
        "                            if all(f\"{base}{suffix}\" in names_set for suffix in ['.t1c.npy', '.t2w.npy', '.t2f.npy']):\n",
        "                                out_f.write(f\"{tar_path}:{base}\\n\")\n",
        "                                out_f.flush() # 强制刷新到磁盘，防止断电丢失\n",
        "                                found_count += 1\n",
        "                    print(f\"  [Step 4/4] Done. Found {found_count} samples.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  [!!! ERROR] {e}\")\n",
        "                if \"Transport endpoint is not connected\" in str(e):\n",
        "                    print(\"\\n[CRITICAL] Google Drive disconnected! Please remount and run again.\")\n",
        "                    break # 这种错误必须停止，重新挂载后再跑\n",
        "            print(\"-\" * 40)\n",
        "else:\n",
        "    print(f\"BraTS tar root not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtnXgthqYd6j",
        "outputId": "c997ccb4-ac09-4983-a498-f5d69022dfb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AR-SSL4M-DEMO'...\n",
            "remote: Enumerating objects: 389, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 389 (delta 74), reused 56 (delta 29), pack-reused 274 (from 1)\u001b[K\n",
            "Receiving objects: 100% (389/389), 1.73 MiB | 12.24 MiB/s, done.\n",
            "Resolving deltas: 100% (194/194), done.\n",
            "/content/AR-SSL4M-DEMO\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository (if not already present)\n",
        "# Cloning from your GitHub repository as requested\n",
        "!git clone https://github.com/tanglehunter00/AR-SSL4M-DEMO.git\n",
        "\n",
        "# IMPORTANT: If you are running this notebook and the code is NOT on Drive,\n",
        "# you need to upload the code files to Colab runtime.\n",
        "\n",
        "project_root = '/content/AR-SSL4M-DEMO'\n",
        "import os\n",
        "if os.path.exists(project_root):\n",
        "    %cd {project_root}\n",
        "else:\n",
        "    print(\"Project root not found. Please clone or upload your code.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRdRFsfUYd6j",
        "outputId": "99452ab2-c6cd-472b-a160-25f8655e4f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Collecting monai\n",
            "  Downloading monai-1.5.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Collecting fire\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.10.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.25.0+cu128)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (1.4.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.24.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire) (3.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch->timm) (1.3.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
            "Downloading monai-1.5.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, monai\n",
            "Successfully installed fire-0.7.1 monai-1.5.2\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install timm monai transformers fire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5oZC_lCYd6j",
        "outputId": "eb1be567-35aa-4d97-ed0f-fb84161bc0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start processing DeepLesion from: /content/drive/MyDrive/dataset/pretrain/DeepLesion/npy\n",
            "------------------------------------------------------------\n",
            "[Step 1/3] Fetching all files from directory... (This may take a while on GDrive)\n",
            "  Done. Total files found in directory: 9816\n",
            "[Step 2/3] Categorizing files by suffix (_1.npy to _8.npy)...\n",
            "  > Suffix _1.npy: Found 247 files. Generating 61 samples.\n",
            "    - Sampling progress:  Done.\n",
            "  > Suffix _2.npy: Found 2176 files. Generating 544 samples.\n",
            "    - Sampling progress:  Done.\n",
            "  > Suffix _3.npy: Found 1672 files. Generating 418 samples.\n",
            "    - Sampling progress:  Done.\n",
            "  > Suffix _4.npy: Found 1284 files. Generating 321 samples.\n",
            "    - Sampling progress:  Done.\n",
            "  > Suffix _5.npy: Found 2394 files. Generating 598 samples.\n",
            "    - Sampling progress:  Done.\n",
            "  > Suffix _6.npy: Found 495 files. Generating 123 samples.\n",
            "    - Sampling progress:  Done.\n",
            "  > Suffix _7.npy: Found 681 files. Generating 170 samples.\n",
            "    - Sampling progress:  Done.\n",
            "  > Suffix _8.npy: Found 867 files. Generating 216 samples.\n",
            "    - Sampling progress:  Done.\n",
            "------------------------------------------------------------\n",
            "[Step 3/3] Writing 2451 samples to /content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists/train_semantic.txt...\n",
            "SUCCESS! DeepLesion semantic list ready.\n"
          ]
        }
      ],
      "source": [
        "# (Optional) Generate DeepLesion Semantic List\n",
        "import os\n",
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "drive_dataset_path = '/content/drive/MyDrive/dataset'\n",
        "npy_dir = os.path.join(drive_dataset_path, 'pretrain', 'DeepLesion', 'npy')\n",
        "list_dir = os.path.join(drive_dataset_path, 'LIDC-IDRI', 'pretrain_lists')\n",
        "os.makedirs(list_dir, exist_ok=True)\n",
        "semantic_list_path = os.path.join(list_dir, 'train_semantic.txt')\n",
        "\n",
        "if os.path.exists(npy_dir):\n",
        "    all_data_list = []\n",
        "    print(f\"Start processing DeepLesion from: {npy_dir}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # 步骤 1: 获取完整文件列表 (这是最耗 IO 的一步)\n",
        "    print(f\"[Step 1/3] Fetching all files from directory... (This may take a while on GDrive)\")\n",
        "    try:\n",
        "        all_files = os.listdir(npy_dir)\n",
        "        print(f\"  Done. Total files found in directory: {len(all_files)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [!!! ERROR] Failed to list directory: {e}\")\n",
        "        all_files = []\n",
        "\n",
        "    if all_files:\n",
        "        # 步骤 2: 按照 1-8 的后缀对文件进行分类\n",
        "        print(f\"[Step 2/3] Categorizing files by suffix (_1.npy to _8.npy)...\")\n",
        "        for num in range(8):\n",
        "            target_suffix = f'_{num+1}.npy'\n",
        "            # 筛选出符合当前后缀的文件\n",
        "            data_list = [os.path.join(npy_dir, x) for x in all_files if x.endswith(target_suffix)]\n",
        "            num_available = len(data_list)\n",
        "\n",
        "            # 计算需要生成的样本数\n",
        "            n_samples = min(20000, num_available // 4) if num_available >= 4 else 0\n",
        "            print(f\"  > Suffix {target_suffix}: Found {num_available} files. Generating {n_samples} samples.\")\n",
        "\n",
        "            # 步骤 3: 随机采样生成组合\n",
        "            if n_samples > 0:\n",
        "                print(f\"    - Sampling progress: \", end=\"\")\n",
        "                for i in range(n_samples):\n",
        "                    choose_list = random.sample(data_list, 4)\n",
        "                    all_data_list.append(','.join(choose_list))\n",
        "\n",
        "                    # 每 5000 个样本打印一次小进度，防止看起来像死机\n",
        "                    if (i + 1) % 5000 == 0:\n",
        "                        print(f\"{i+1}..\", end=\"\", flush=True)\n",
        "                print(f\" Done.\")\n",
        "            else:\n",
        "                print(f\"    - Skipping (not enough files).\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        # 步骤 4: 保存结果\n",
        "        print(f\"[Step 3/3] Writing {len(all_data_list)} samples to {semantic_list_path}...\")\n",
        "        try:\n",
        "            with open(semantic_list_path, 'w') as f:\n",
        "                f.write('\\n'.join(all_data_list))\n",
        "            print(f\"SUCCESS! DeepLesion semantic list ready.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  [!!! ERROR] Failed to write file: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"DeepLesion npy dir not found: {npy_dir}. Skip semantic list.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebMGTeF4dKYP",
        "outputId": "d86ffc0d-b4da-4643-d4dc-6356c4ee37be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created training list at /content/drive/MyDrive/dataset/LIDC-IDRI/colab_train_list.txt with 24850 files.\n"
          ]
        }
      ],
      "source": [
        "# 旧版本训练\n",
        "# Update dataset configuration paths dynamically\n",
        "# We need to point the dataset config to the list files in Google Drive\n",
        "\n",
        "# Assuming your list files are also in the dataset folder on Drive\n",
        "# You might need to generate these list files if they contain absolute local paths from your PC.\n",
        "# Here we create a new list file based on the Drive path.\n",
        "\n",
        "import os\n",
        "\n",
        "drive_dataset_path = '/content/drive/MyDrive/dataset/LIDC-IDRI'\n",
        "patch_dir = os.path.join(drive_dataset_path, 'patch_random_spatial')\n",
        "list_file_path = os.path.join(drive_dataset_path, 'colab_train_list.txt')\n",
        "\n",
        "if os.path.exists(patch_dir):\n",
        "    npy_files = [f for f in os.listdir(patch_dir) if f.endswith('.npy')]\n",
        "    full_paths = [os.path.join(patch_dir, f) for f in npy_files]\n",
        "\n",
        "    with open(list_file_path, 'w') as f:\n",
        "        f.write('\\n'.join(full_paths))\n",
        "    print(f\"Created training list at {list_file_path} with {len(full_paths)} files.\")\n",
        "else:\n",
        "    print(\"Patch directory not found. Please ensure 'patch_random_spatial' exists inside 'dataset/demo'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g5ilPlqdL7H",
        "outputId": "939711f3-03db-4f72-b22d-1f75967afcab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USE_MODE=all, add_series_data=True\n",
            "  样本数: spatial=24850, contrast=124100, semantic=2451, 合计≈151401\n"
          ]
        }
      ],
      "source": [
        "# Modify newFullPretrain/configs/datasets.py to use generated list paths\n",
        "\n",
        "import os\n",
        "\n",
        "# ========== 选择使用的数据：改这里即可 ==========\n",
        "USE_MODE = \"all\"  # 可选: \"lidc_only\" | \"lidc_brats\" | \"lidc_deeplesion\" | \"brats_only\" | \"all\"\n",
        "# ============================================\n",
        "\n",
        "list_dir = '/content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists'\n",
        "os.makedirs(list_dir, exist_ok=True)\n",
        "spatial_path = os.path.join(list_dir, 'train_spatial.txt')\n",
        "contrast_path = os.path.join(list_dir, 'train_contrast.txt')\n",
        "semantic_path = os.path.join(list_dir, 'train_semantic.txt')\n",
        "\n",
        "# 占位空文件：用于“排除”某个数据源时指向这里，不覆盖真实数据\n",
        "empty_path = os.path.join(list_dir, '_empty.txt')\n",
        "if not os.path.exists(empty_path):\n",
        "    open(empty_path, 'w').close()\n",
        "\n",
        "# 根据 USE_MODE 决定实际使用的路径（不修改原始文件）\n",
        "if USE_MODE == \"lidc_only\":\n",
        "    effective_contrast = empty_path\n",
        "    effective_semantic = empty_path\n",
        "elif USE_MODE == \"lidc_brats\":\n",
        "    effective_contrast = contrast_path\n",
        "    effective_semantic = empty_path\n",
        "elif USE_MODE == \"lidc_deeplesion\":\n",
        "    effective_contrast = empty_path\n",
        "    effective_semantic = semantic_path\n",
        "elif USE_MODE == \"brats_only\":\n",
        "    effective_contrast = contrast_path\n",
        "    effective_semantic = empty_path\n",
        "else:  # \"all\"\n",
        "    effective_contrast = contrast_path\n",
        "    effective_semantic = semantic_path\n",
        "\n",
        "add_series_data = (os.path.getsize(effective_contrast) > 0) or (os.path.getsize(effective_semantic) > 0)\n",
        "add_spatial_data = False if USE_MODE == \"brats_only\" else True\n",
        "\n",
        "project_root = '/content/AR-SSL4M-DEMO'\n",
        "config_path = os.path.join(project_root, 'newFullPretrain', 'configs', 'datasets.py')\n",
        "new_config_content = f\"\"\"\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class custom_dataset:\n",
        "    dataset: str = \"custom_dataset\"\n",
        "    file: str = \"image_dataset.py\"\n",
        "    train_split: str = \"train\"\n",
        "    test_split: str = \"validation\"\n",
        "    spatial_path: str = \"{spatial_path}\"\n",
        "    contrast_path: str = \"{effective_contrast}\"\n",
        "    semantic_path: str = \"{effective_semantic}\"\n",
        "    img_size = [128, 128, 128]\n",
        "    patch_size = [16, 16, 16]\n",
        "    attention_type = 'prefix'\n",
        "    add_series_data = {str(add_series_data)}\n",
        "    add_spatial_data = {str(add_spatial_data)}\n",
        "    is_subset = False\n",
        "    series_length = 4\n",
        "\"\"\"\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(new_config_content)\n",
        "\n",
        "n_spatial = len(open(spatial_path).readlines()) if os.path.exists(spatial_path) else 0\n",
        "n_contrast = len(open(effective_contrast).readlines()) if os.path.getsize(effective_contrast) > 0 else 0\n",
        "n_semantic = len(open(effective_semantic).readlines()) if os.path.getsize(effective_semantic) > 0 else 0\n",
        "total = n_spatial + (n_contrast + n_semantic if add_series_data else 0)\n",
        "\n",
        "print(f\"USE_MODE={USE_MODE}, add_series_data={add_series_data}\")\n",
        "print(f\"  样本数: spatial={n_spatial}, contrast={n_contrast}, semantic={n_semantic}, 合计≈{total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf8E-fLkYd6j",
        "outputId": "a51227f4-d700-4061-92b8-5eeba88351aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated newFullPretrain config. add_series_data=True\n"
          ]
        }
      ],
      "source": [
        "# Modify newFullPretrain/configs/datasets.py to use generated list paths\n",
        "\n",
        "import os\n",
        "\n",
        "list_dir = '/content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists'\n",
        "os.makedirs(list_dir, exist_ok=True)\n",
        "spatial_path = os.path.join(list_dir, 'train_spatial.txt')\n",
        "contrast_path = os.path.join(list_dir, 'train_contrast.txt')\n",
        "semantic_path = os.path.join(list_dir, 'train_semantic.txt')\n",
        "\n",
        "# Create empty files if contrast/semantic lists don't exist (dataset expects readable files)\n",
        "for p in [contrast_path, semantic_path]:\n",
        "    if not os.path.exists(p):\n",
        "        open(p, 'w').close()\n",
        "\n",
        "add_series_data = (os.path.getsize(contrast_path) > 0) or (os.path.getsize(semantic_path) > 0)\n",
        "\n",
        "project_root = '/content/AR-SSL4M-DEMO'\n",
        "config_path = os.path.join(project_root, 'newFullPretrain', 'configs', 'datasets.py')\n",
        "new_config_content = f\"\"\"\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class custom_dataset:\n",
        "    dataset: str = \"custom_dataset\"\n",
        "    file: str = \"image_dataset.py\"\n",
        "    train_split: str = \"train\"\n",
        "    test_split: str = \"validation\"\n",
        "    spatial_path: str = \"{spatial_path}\"\n",
        "    contrast_path: str = \"{contrast_path}\"\n",
        "    semantic_path: str = \"{semantic_path}\"\n",
        "    img_size = [128, 128, 128]\n",
        "    patch_size = [16, 16, 16]\n",
        "    attention_type = 'prefix'\n",
        "    add_series_data = {str(add_series_data)}\n",
        "    add_spatial_data = True\n",
        "    is_subset = False\n",
        "    series_length = 4\n",
        "\"\"\"\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(new_config_content)\n",
        "\n",
        "print(f\"Updated newFullPretrain config. add_series_data={add_series_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ino-Du0PYd6k"
      },
      "outputs": [],
      "source": [
        "# Run Pretraining (using newFullPretrain - supports tar.gz BraTS, LIDC, DeepLesion)\n",
        "\n",
        "%cd newFullPretrain\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/dataset/LIDC-IDRI/output\n",
        "\n",
        "!python main.py \\\n",
        "    --enable_fsdp False \\\n",
        "    --output_dir /content/drive/MyDrive/dataset/LIDC-IDRI/output \\\n",
        "    --batch_size_training 64 \\\n",
        "    --num_epochs 1 \\\n",
        "    --save_metrics True \\\n",
        "    --num_workers_dataloader 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9F15-jtgiY7",
        "outputId": "1970a54e-7cfd-4848-ae05-9f34a59a198a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "from dataclasses import dataclass\n",
            "\n",
            "@dataclass\n",
            "class custom_dataset:\n",
            "    dataset: str = \"custom_dataset\"\n",
            "    file: str = \"image_dataset.py\"\n",
            "    train_split: str = \"train\"\n",
            "    test_split: str = \"validation\"\n",
            "    spatial_path: str = \"/content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists/train_spatial.txt\"\n",
            "    contrast_path: str = \"/content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists/train_spatial.txt\"\n",
            "    semantic_path: str = \"/content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists/train_spatial.txt\"\n",
            "    img_size = [128, 128, 128]\n",
            "    patch_size = [16, 16, 16]\n",
            "    attention_type = 'prefix'\n",
            "    add_series_data = True\n",
            "    add_spatial_data = True\n",
            "    is_subset = False\n",
            "    series_length = 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "config_path = '/content/AR-SSL4M-DEMO/newFullPretrain/configs/datasets.py'\n",
        "if os.path.exists(config_path):\n",
        "    print(open(config_path).read())\n",
        "else:\n",
        "    print(f\"文件不存在: {config_path}\")\n",
        "    print(\"当前目录:\", os.getcwd())\n",
        "    print(\"目录内容:\", os.listdir('/content/AR-SSL4M-DEMO'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2sYa3YCkO-A",
        "outputId": "5dd43748-2302-4c22-d823-fefb5714f569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spatial: 24850 行\n",
            "contrast: 0 行\n",
            "semantic: 0 行\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "list_dir = '/content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists'\n",
        "paths = {\n",
        "    'spatial':  os.path.join(list_dir, 'train_spatial.txt'),\n",
        "    'contrast': os.path.join(list_dir, 'train_contrast.txt'),\n",
        "    'semantic': os.path.join(list_dir, 'train_semantic.txt'),\n",
        "}\n",
        "\n",
        "for name, p in paths.items():\n",
        "    if os.path.exists(p):\n",
        "        with open(p, 'r') as f:\n",
        "            n = len(f.readlines())\n",
        "        print(f\"{name}: {n} 行\")\n",
        "    else:\n",
        "        print(f\"{name}: 文件不存在\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oBun6vUlsqq",
        "outputId": "b0a6a977-3478-4046-e301-0ad74244a367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_contrast.txt 行数: 0\n"
          ]
        }
      ],
      "source": [
        "   with open('/content/drive/MyDrive/dataset/LIDC-IDRI/pretrain_lists/train_contrast.txt') as f:\n",
        "       lines = f.readlines()\n",
        "   print(f\"train_contrast.txt 行数: {len(lines)}\")\n",
        "   if lines:\n",
        "       print(f\"首行示例: {lines[0][:80]}...\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
