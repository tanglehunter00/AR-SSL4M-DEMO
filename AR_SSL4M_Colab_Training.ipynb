{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmIG5j70gTE7"
      },
      "source": [
        "# AR-SSL4M Pretraining on Google Colab 1\n",
        "\n",
        "This notebook runs AR-SSL4M (Autoregressive Sequence Modeling for 3D Medical Image Representation) pretraining on Google Colab.\n",
        "\n",
        "## Requirements\n",
        "- GPU runtime (T4, V100, or A100)\n",
        "- High RAM runtime (recommended)\n",
        "\n",
        "## Dataset\n",
        "- Using STOIC dataset (2771 samples)\n",
        "- Each sample: 128Ã—128Ã—128 3D medical images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTXSugc3gTE9"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "**âš ï¸ é‡è¦æç¤º**: å¦‚æžœé‡åˆ°ä¾èµ–å†²çªé”™è¯¯ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n",
        "1. é‡å¯è¿è¡Œæ—¶ (Runtime â†’ Restart Runtime)\n",
        "2. é‡æ–°è¿è¡Œæ‰€æœ‰cells\n",
        "3. å¦‚æžœä»æœ‰é—®é¢˜ï¼Œè¯·ä½¿ç”¨ \"Factory Reset Runtime\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jMHEOFqkgTE9",
        "outputId": "4212d692-9d78-4684-f529-4b6ae80ca06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU detected! Please enable GPU runtime in Colab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4THEn0SFgTE-",
        "outputId": "cf7958c8-64a3-4faf-f96b-c7d7dc24a863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ æ™ºèƒ½æ£€æµ‹ColabçŽ¯å¢ƒ...\n",
            "ðŸ”¥ æ£€æµ‹åˆ°PyTorchç‰ˆæœ¬: 2.8.0+cu126\n",
            "ðŸ”¥ CUDAå¯ç”¨: True\n",
            "ðŸ” PyTorchç›¸å…³åŒ…ç‰ˆæœ¬:\n",
            "  torch                                 2.8.0+cu126\n",
            "  torchaudio                            2.8.0+cu126\n",
            "  torchvision                           0.23.0+cu126\n",
            "ðŸ”§ ç¡®ä¿PyTorchç»„ä»¶ç‰ˆæœ¬ä¸€è‡´: 2.8.0+cu126\n",
            "ðŸ“š å®‰è£…è½»é‡çº§å¿…éœ€åŒ…...\n",
            "ðŸ“¥ å®‰è£… fire...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… tqdm å·²å¯ç”¨\n",
            "âœ… PyYAML å·²å¯ç”¨\n",
            "âœ… packaging å·²å¯ç”¨\n",
            "âœ… Transformers: 4.56.1\n",
            "ðŸ“¥ å®‰è£…MONAIï¼ˆä¸æ›´æ”¹PyTorchï¼‰...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… æ™ºèƒ½å®‰è£…å®Œæˆ!\n",
            "ðŸŽ¯ ç­–ç•¥ï¼šä¿æŒPyTorchç»„ä»¶ç‰ˆæœ¬ä¸€è‡´\n"
          ]
        }
      ],
      "source": [
        "# æ™ºèƒ½æ£€æµ‹å¹¶ä¿®å¤PyTorchç‰ˆæœ¬é—®é¢˜\n",
        "print(\"ðŸ“¦ æ™ºèƒ½æ£€æµ‹ColabçŽ¯å¢ƒ...\")\n",
        "\n",
        "# å…ˆæ£€æŸ¥PyTorchç‰ˆæœ¬ä¸€è‡´æ€§\n",
        "import torch\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(f\"ðŸ”¥ æ£€æµ‹åˆ°PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"ðŸ”¥ CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç‰ˆæœ¬ä¸ä¸€è‡´é—®é¢˜\n",
        "result = subprocess.run([sys.executable, '-m', 'pip', 'list'],\n",
        "                       capture_output=True, text=True)\n",
        "pip_list = result.stdout\n",
        "\n",
        "torch_versions = []\n",
        "for line in pip_list.split('\\n'):\n",
        "    if line.startswith('torch ') or line.startswith('torchaudio ') or line.startswith('torchvision '):\n",
        "        torch_versions.append(line.strip())\n",
        "\n",
        "print(\"ðŸ” PyTorchç›¸å…³åŒ…ç‰ˆæœ¬:\")\n",
        "for version in torch_versions:\n",
        "    print(f\"  {version}\")\n",
        "\n",
        "# å¦‚æžœæ£€æµ‹åˆ°ç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œä¿®å¤å®ƒ\n",
        "if len(torch_versions) > 0:\n",
        "    # èŽ·å–å½“å‰çŽ¯å¢ƒçš„å®žé™…torchç‰ˆæœ¬\n",
        "    current_torch = torch.__version__\n",
        "    if '+' in current_torch:\n",
        "        base_version = current_torch.split('+')[0]\n",
        "        cuda_suffix = current_torch.split('+')[1]\n",
        "    else:\n",
        "        base_version = current_torch\n",
        "        cuda_suffix = 'cu126'  # Colabé»˜è®¤\n",
        "\n",
        "    print(f\"ðŸ”§ ç¡®ä¿PyTorchç»„ä»¶ç‰ˆæœ¬ä¸€è‡´: {base_version}+{cuda_suffix}\")\n",
        "\n",
        "    # é‡æ–°å®‰è£…ä¸€è‡´çš„PyTorchå¥—ä»¶ï¼ˆä½¿ç”¨Colabçš„ç´¢å¼•ï¼‰\n",
        "    !pip install -q --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# å®‰è£…è½»é‡çº§å¿…éœ€åŒ…\n",
        "print(\"ðŸ“š å®‰è£…è½»é‡çº§å¿…éœ€åŒ…...\")\n",
        "lightweight_packages = ['fire', 'tqdm', 'PyYAML', 'packaging']\n",
        "\n",
        "for pkg in lightweight_packages:\n",
        "    try:\n",
        "        if pkg == 'PyYAML':\n",
        "            import yaml\n",
        "            print(f\"âœ… {pkg} å·²å¯ç”¨\")\n",
        "        else:\n",
        "            __import__(pkg)\n",
        "            print(f\"âœ… {pkg} å·²å¯ç”¨\")\n",
        "    except ImportError:\n",
        "        print(f\"ðŸ“¥ å®‰è£… {pkg}...\")\n",
        "        !pip install -q {pkg}\n",
        "\n",
        "# æ£€æŸ¥transformersï¼ˆé€šå¸¸Colabå·²æœ‰ï¼‰\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"âœ… Transformers: {transformers.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"ðŸ“¥ å®‰è£…Transformers...\")\n",
        "    !pip install -q transformers\n",
        "\n",
        "# æœ€åŽå®‰è£…MONAIï¼ˆå¦‚æžœéœ€è¦ï¼‰\n",
        "try:\n",
        "    import monai\n",
        "    print(f\"âœ… MONAI: {monai.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"ðŸ“¥ å®‰è£…MONAIï¼ˆä¸æ›´æ”¹PyTorchï¼‰...\")\n",
        "    !pip install -q --no-deps monai\n",
        "    # å•ç‹¬å®‰è£…MONAIçš„å¿…éœ€ä¾èµ–ï¼ˆé¿å…PyTorchç‰ˆæœ¬å†²çªï¼‰\n",
        "    !pip install -q nibabel tqdm\n",
        "\n",
        "print(\"âœ… æ™ºèƒ½å®‰è£…å®Œæˆ!\")\n",
        "print(\"ðŸŽ¯ ç­–ç•¥ï¼šä¿æŒPyTorchç»„ä»¶ç‰ˆæœ¬ä¸€è‡´\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjQzvJ5gTE-"
      },
      "source": [
        "## 2. Clone Repository and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_AFgK1x4gTE-",
        "outputId": "4272b46a-bd73-4b84-f75e-e0f6ebd77bf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository...\n",
            "Cloning into 'AR-SSL4M-DEMO'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 161 (delta 56), reused 137 (delta 36), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (161/161), 1.32 MiB | 12.51 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "âœ… Current directory: /content/AR-SSL4M-DEMO\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "repo_url = \"https://github.com/tanglehunter00/AR-SSL4M-DEMO.git\"\n",
        "\n",
        "if os.path.exists(\"AR-SSL4M-DEMO\"):\n",
        "    print(\"Repository already exists, pulling latest changes...\")\n",
        "    !cd AR-SSL4M-DEMO && git pull\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone {repo_url}\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(\"AR-SSL4M-DEMO\")\n",
        "print(f\"âœ… Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84Mu0xysgTE-"
      },
      "source": [
        "## 3. Data Setup\n",
        "\n",
        "æ ¹æ®æ‚¨çš„æˆªå›¾ï¼Œæ•°æ®é›†ä½äºŽGoogle Driveçš„ `dataset/compressed_datasets/volumes` è·¯å¾„ä¸­ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tfhUXjRLgTE-",
        "outputId": "5b3f0d5f-8720-49ab-d842-1a3b70465bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Found 13950 .npy files in /content/drive/MyDrive/dataset/volumes\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access your data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to your data in Google Drive (based on your screenshot)\n",
        "data_path = \"/content/drive/MyDrive/dataset/volumes\"\n",
        "\n",
        "# Check if data exists\n",
        "if os.path.exists(data_path):\n",
        "    npy_files = [f for f in os.listdir(data_path) if f.endswith('.npy')]\n",
        "    print(f\"âœ… Found {len(npy_files)} .npy files in {data_path}\")\n",
        "    if len(npy_files) == 0:\n",
        "        print(\"âš ï¸ No .npy files found in the directory\")\n",
        "        print(\"Available files:\")\n",
        "        all_files = os.listdir(data_path)[:10]  # Show first 10 files\n",
        "        for f in all_files:\n",
        "            print(f\"  - {f}\")\n",
        "else:\n",
        "    print(f\"âŒ Data path not found: {data_path}\")\n",
        "    print(\"Available paths in Google Drive:\")\n",
        "    try:\n",
        "        drive_contents = os.listdir(\"/content/drive/MyDrive\")\n",
        "        for item in drive_contents[:10]:\n",
        "            print(f\"  - /content/drive/MyDrive/{item}\")\n",
        "    except:\n",
        "        print(\"Unable to list drive contents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kr5ocnzIgTE-",
        "outputId": "c271d43a-b30d-407f-d3f3-e7569f914fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found 13950 .npy files in /content/drive/MyDrive/dataset/volumes\n",
            "âœ… Created data list: pretrain/colab_data_list.txt with 13950 files\n"
          ]
        }
      ],
      "source": [
        "# Create data list file\n",
        "import glob\n",
        "\n",
        "# Try multiple possible data paths\n",
        "possible_paths = [\n",
        "    \"/content/drive/MyDrive/dataset/compressed_datasets/volumes\",\n",
        "    \"/content/drive/MyDrive/compressed_datasets/volumes\",\n",
        "    \"/content/drive/MyDrive/dataset/volumes\",\n",
        "    \"/content/drive/MyDrive/volumes\"\n",
        "]\n",
        "\n",
        "npy_files = []\n",
        "actual_data_path = None\n",
        "\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        files = glob.glob(os.path.join(path, \"*.npy\"))\n",
        "        if files:\n",
        "            npy_files = files\n",
        "            actual_data_path = path\n",
        "            print(f\"âœ… Found {len(npy_files)} .npy files in {actual_data_path}\")\n",
        "            break\n",
        "        else:\n",
        "            print(f\"âš ï¸ Path exists but no .npy files found: {path}\")\n",
        "\n",
        "if not npy_files:\n",
        "    print(\"âŒ No .npy files found in any of the expected paths\")\n",
        "    print(\"Please check your Google Drive structure and update the paths accordingly\")\n",
        "else:\n",
        "    # Create data list file\n",
        "    data_list_path = \"pretrain/colab_data_list.txt\"\n",
        "    with open(data_list_path, 'w') as f:\n",
        "        for npy_file in npy_files:\n",
        "            f.write(f\"{npy_file}\\n\")\n",
        "\n",
        "    print(f\"âœ… Created data list: {data_list_path} with {len(npy_files)} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R74wqFSNgTE_"
      },
      "source": [
        "## 4. Update Configuration for Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Wx0ccfbgTE_",
        "outputId": "ada421c6-f6ae-4479-ec65-4724d0ec0bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Updated dataset configuration\n"
          ]
        }
      ],
      "source": [
        "# Update dataset configuration\n",
        "config_content = '''from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class custom_dataset:\n",
        "    dataset: str = \"custom_dataset\"\n",
        "    file: str = \"image_dataset.py\"\n",
        "    train_split: str = \"train\"\n",
        "    test_split: str = \"validation\"\n",
        "    spatial_path: str = \"colab_data_list.txt\"\n",
        "    contrast_path: str = \"colab_data_list.txt\"\n",
        "    semantic_path: str = \"colab_data_list.txt\"\n",
        "    img_size = [128, 128, 128]\n",
        "    patch_size = [16, 16, 16]\n",
        "    attention_type = 'prefix'\n",
        "    add_series_data = False\n",
        "    add_spatial_data = True\n",
        "    is_subset = False\n",
        "    series_length = 4\n",
        "'''\n",
        "\n",
        "with open('pretrain/configs/datasets.py', 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"âœ… Updated dataset configuration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LVMNGSlwgTE_",
        "outputId": "ff7fbf0d-106d-4bdc-fdb8-597b1f59539d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Updated training configuration for Colab\n"
          ]
        }
      ],
      "source": [
        "# Update training configuration for Colab\n",
        "training_config_content = '''from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class train_config:\n",
        "    enable_fsdp: bool=False\n",
        "    low_cpu_fsdp: bool=False\n",
        "    run_validation: bool=True\n",
        "    batch_size_training: int=4  # Adjusted for Colab GPU memory\n",
        "    batching_strategy: str=\"padding\"\n",
        "    gradient_accumulation_steps: int=1\n",
        "    gradient_clipping: bool=False\n",
        "    gradient_clipping_threshold: float = 1.0\n",
        "    num_epochs: int=1  # Single epoch for Colab\n",
        "    warmup_epochs:int=0\n",
        "    num_workers_dataloader: int=0  # Set to 0 to avoid multiprocessing issues in Colab\n",
        "    lr: float=1e-4\n",
        "    weight_decay: float=0.01\n",
        "    gamma: float=0.1\n",
        "    seed: int=42\n",
        "    use_fp16: bool=True  # Enable FP16 for Colab GPU memory efficiency\n",
        "    mixed_precision: bool=True\n",
        "    val_batch_size: int=1\n",
        "    dataset = \"custom_dataset\"\n",
        "    output_dir: str=\"/content/AR-SSL4M-DEMO/pretrain/save\"\n",
        "    freeze_layers: bool=False\n",
        "    num_freeze_layers: int=1\n",
        "    save_model: bool=True\n",
        "    save_optimizer: bool=False\n",
        "    save_metrics: bool=True\n",
        "    scheduler:str='CosineLR'\n",
        "    min_lr: float=0\n",
        "    pos_type: str='sincos3d'\n",
        "    norm_pixel_loss: bool=True\n",
        "    enable_profiling: bool=False  # Disable profiling for cleaner output\n",
        "'''\n",
        "\n",
        "with open('pretrain/configs/training.py', 'w') as f:\n",
        "    f.write(training_config_content)\n",
        "\n",
        "print(\"âœ… Updated training configuration for Colab\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM1yOzZjgTE_"
      },
      "source": [
        "## 5. Start Pretraining\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykGSA8AYgTE_"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5gHNyMjmgTE_",
        "outputId": "36e9b5ed-4652-460e-db5a-7b4d91cf8f91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created save directory\n"
          ]
        }
      ],
      "source": [
        "# Create save directory\n",
        "os.makedirs(\"pretrain/save\", exist_ok=True)\n",
        "print(\"âœ… Created save directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7FUID4mgTE_",
        "outputId": "07ca0469-37bf-4582-c117-51437aeeea74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting AR-SSL4M Pretraining...\n",
            "============================================================\n",
            "2025-09-21 09:10:07.386865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758445807.413575    2764 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758445807.421514    2764 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758445807.440899    2764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758445807.440922    2764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758445807.440930    2764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758445807.440936    2764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-21 09:10:07.446855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/AR-SSL4M-DEMO/pretrain/utils/model_checkpointing_utils.py:15: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
            "  from torch.distributed._shard.checkpoint import (\n",
            "--> Model has 91.327744 Million params\n",
            "\n",
            "--> Training Set Length = 13922\n",
            "--> Validation Set Length = 28\n",
            "Training Epoch: 1/1, step 89/3480 completed (loss: 0.7776):   3% 89/3480 [05:09<3:09:56,  3.36s/it]"
          ]
        }
      ],
      "source": [
        "# Start pretraining\n",
        "print(\"ðŸš€ Starting AR-SSL4M Pretraining...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Change to pretrain directory and run\n",
        "os.chdir(\"pretrain\")\n",
        "\n",
        "# Run the training script\n",
        "!python main.py --output_dir save --batch_size_training 4\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… Pretraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViUgmLNFgTE_"
      },
      "source": [
        "## 6. Check Results and Download Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FojvBPSgTFA"
      },
      "outputs": [],
      "source": [
        "# Check training results\n",
        "import os\n",
        "import glob\n",
        "\n",
        "save_dir = \"save\"\n",
        "if os.path.exists(save_dir):\n",
        "    files = os.listdir(save_dir)\n",
        "    print(f\"ðŸ“ Files in save directory:\")\n",
        "    for file in files:\n",
        "        file_path = os.path.join(save_dir, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "            print(f\"  - {file} ({size_mb:.1f} MB)\")\n",
        "else:\n",
        "    print(\"âŒ Save directory not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAm5OYFhgTFA"
      },
      "outputs": [],
      "source": [
        "# Download the trained model\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# Create a zip file with all results\n",
        "zip_path = \"/content/ar_ssl4m_pretrained_model.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    for root, dirs, files_list in os.walk(\"save\"):\n",
        "        for file in files_list:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, \"save\")\n",
        "            zipf.write(file_path, arcname)\n",
        "            print(f\"Added to zip: {arcname}\")\n",
        "\n",
        "print(f\"\\nðŸ“¦ Created zip file: {zip_path}\")\n",
        "print(\"â¬‡ï¸ Downloading...\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObDNh-ImgTFA"
      },
      "source": [
        "## 7. Training Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG1SIj6egTFA"
      },
      "outputs": [],
      "source": [
        "# Display training summary\n",
        "print(\"ðŸŽ¯ AR-SSL4M Pretraining Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ðŸ“Š Dataset: STOIC ({len(npy_files) if 'npy_files' in locals() and npy_files else 'Unknown'} samples)\")\n",
        "print(f\"ðŸ“‚ Data path: {actual_data_path if 'actual_data_path' in locals() and actual_data_path else 'Not found'}\")\n",
        "print(f\"ðŸ—ï¸ Model: AR-SSL4M (91.3M parameters)\")\n",
        "print(f\"ðŸ“ Image size: 128Ã—128Ã—128\")\n",
        "print(f\"ðŸ”¢ Patch size: 16Ã—16Ã—16\")\n",
        "print(f\"ðŸ“¦ Batch size: 4\")\n",
        "print(f\"ðŸŽ“ Learning rate: 1e-4\")\n",
        "print(f\"ðŸ”„ Epochs: 1\")\n",
        "print(f\"ðŸ’¾ Results saved to: save/\")\n",
        "print(f\"ðŸ”— GitHub Repo: https://github.com/tanglehunter00/AR-SSL4M-DEMO\")\n",
        "print(\"=\" * 50)\n",
        "print(\"âœ… Training completed successfully!\")\n",
        "print(\"\\nðŸ“ Next steps:\")\n",
        "print(\"1. Download the model zip file\")\n",
        "print(\"2. Use the pretrained model for downstream tasks\")\n",
        "print(\"3. Fine-tune on specific medical imaging tasks (segmentation, classification, etc.)\")\n",
        "print(\"4. Experiment with different hyperparameters for better performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jImRZQY6gTFA"
      },
      "source": [
        "https://github.com/tanglehunter00/AR-SSL4M-DEMO.git"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}