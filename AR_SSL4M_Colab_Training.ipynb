{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AR-SSL4M Pretraining on Google Colab\n",
        "\n",
        "This notebook runs AR-SSL4M (Autoregressive Sequence Modeling for 3D Medical Image Representation) pretraining on Google Colab.\n",
        "\n",
        "## Requirements\n",
        "- GPU runtime (T4, V100, or A100)\n",
        "- High RAM runtime (recommended)\n",
        "\n",
        "## Dataset\n",
        "- Using STOIC dataset (2771 samples)\n",
        "- Each sample: 128×128×128 3D medical images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "**⚠️ 重要提示**: 如果遇到依赖冲突错误，请按以下步骤操作：\n",
        "1. 重启运行时 (Runtime → Restart Runtime)\n",
        "2. 重新运行所有cells\n",
        "3. 如果仍有问题，请使用 \"Factory Reset Runtime\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠️ No GPU detected! Please enable GPU runtime in Colab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用Google Colab原生支持的包版本\n",
        "print(\"📦 使用Colab原生支持的包版本...\")\n",
        "\n",
        "# 检查当前PyTorch版本\n",
        "import torch\n",
        "print(f\"🔥 当前PyTorch版本: {torch.__version__}\")\n",
        "print(f\"🔥 CUDA可用: {torch.cuda.is_available()}\")\n",
        "\n",
        "# 只安装我们实际需要的最小依赖集\n",
        "print(\"📚 安装训练所需的最小依赖...\")\n",
        "\n",
        "# 安装transformers - 使用Colab兼容版本\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"✅ Transformers已安装: {transformers.__version__}\")\n",
        "    # 如果版本太低，升级到兼容版本\n",
        "    if int(transformers.__version__.split('.')[0]) < 4 or int(transformers.__version__.split('.')[1]) < 30:\n",
        "        print(\"📥 升级Transformers到兼容版本...\")\n",
        "        !pip install -q --upgrade transformers>=4.30.0\n",
        "except ImportError:\n",
        "    print(\"📥 安装Transformers...\")\n",
        "    !pip install -q transformers>=4.30.0\n",
        "\n",
        "# 安装MONAI - 使用最简版本\n",
        "try:\n",
        "    import monai\n",
        "    print(f\"✅ MONAI已安装: {monai.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"📥 安装MONAI核心版本...\")\n",
        "    # 只安装核心MONAI，避免复杂依赖\n",
        "    !pip install -q monai\n",
        "\n",
        "# 安装其他必需的轻量级依赖\n",
        "missing_packages = []\n",
        "required_packages = ['fire', 'tqdm', 'PyYAML', 'packaging']\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package.lower().replace('-', '_'))\n",
        "        print(f\"✅ {package} 已可用\")\n",
        "    except ImportError:\n",
        "        missing_packages.append(package)\n",
        "\n",
        "if missing_packages:\n",
        "    print(f\"📥 安装缺失的包: {', '.join(missing_packages)}\")\n",
        "    import subprocess\n",
        "    for pkg in missing_packages:\n",
        "        subprocess.run(['pip', 'install', '-q', pkg], check=True)\n",
        "\n",
        "# 检查nibabel（医学图像处理）\n",
        "try:\n",
        "    import nibabel\n",
        "    print(\"✅ nibabel 已可用\")\n",
        "except ImportError:\n",
        "    print(\"📥 安装nibabel...\")\n",
        "    !pip install -q nibabel\n",
        "\n",
        "print(\"✅ 使用Colab原生环境完成安装!\")\n",
        "print(\"🎯 重点：使用现有PyTorch版本，避免版本冲突\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installations with detailed error handling\n",
        "print(\"🔍 Verifying installations...\")\n",
        "\n",
        "packages_status = {}\n",
        "\n",
        "# Test each package individually\n",
        "try:\n",
        "    import torch\n",
        "    packages_status['PyTorch'] = f\"✅ {torch.__version__}\"\n",
        "    print(f\"✅ PyTorch: {torch.__version__} (CUDA: {torch.cuda.is_available()})\")\n",
        "except ImportError as e:\n",
        "    packages_status['PyTorch'] = f\"❌ Failed: {e}\"\n",
        "    print(f\"❌ PyTorch import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import transformers\n",
        "    packages_status['Transformers'] = f\"✅ {transformers.__version__}\"\n",
        "    print(f\"✅ Transformers: {transformers.__version__}\")\n",
        "except ImportError as e:\n",
        "    packages_status['Transformers'] = f\"❌ Failed: {e}\"\n",
        "    print(f\"❌ Transformers import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import monai\n",
        "    packages_status['MONAI'] = f\"✅ {monai.__version__}\"\n",
        "    print(f\"✅ MONAI: {monai.__version__}\")\n",
        "except ImportError as e:\n",
        "    packages_status['MONAI'] = f\"❌ Failed: {e}\"\n",
        "    print(f\"❌ MONAI import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import fire, sklearn, tqdm\n",
        "    print(f\"✅ Other packages: fire, sklearn, tqdm\")\n",
        "    packages_status['Others'] = \"✅ OK\"\n",
        "except ImportError as e:\n",
        "    packages_status['Others'] = f\"❌ Failed: {e}\"\n",
        "    print(f\"❌ Other packages import failed: {e}\")\n",
        "\n",
        "# Check if all critical packages are working\n",
        "critical_failed = [k for k, v in packages_status.items() if \"❌\" in v and k in ['PyTorch', 'Transformers', 'MONAI']]\n",
        "\n",
        "if not critical_failed:\n",
        "    print(f\"\\n🎉 All critical dependencies verified successfully!\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ Failed packages: {', '.join(critical_failed)}\")\n",
        "    print(\"💡 Solutions:\")\n",
        "    print(\"1. 如果遇到编译错误，运行 Cell 6 (最简化安装)\")\n",
        "    print(\"2. 如果是MONAI问题，运行 Cell 8 (MONAI专用修复)\")\n",
        "    print(\"3. 重启runtime后重试\")\n",
        "    print(\"4. 使用 'Factory Reset Runtime' 如果问题持续\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 新策略：使用Colab原生环境\n",
        "\n",
        "**问题**: 强制安装特定版本会导致大量依赖冲突\n",
        "\n",
        "**解决方案**: \n",
        "- ✅ 使用Colab已有的PyTorch版本（通常是最新稳定版）\n",
        "- ✅ 只安装缺失的轻量级包\n",
        "- ✅ 避免版本降级和复杂依赖\n",
        "\n",
        "**如果上面的安装仍有问题，运行下面的极简方案**：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 极简安装方案 - 仅安装训练必需的包\n",
        "print(\"🔄 极简安装方案（仅安装必需包）...\")\n",
        "\n",
        "# 保持现有PyTorch版本，不做任何更改\n",
        "import torch\n",
        "print(f\"🔥 使用现有PyTorch: {torch.__version__}\")\n",
        "\n",
        "# 只安装绝对必需的包\n",
        "print(\"📦 安装必需的轻量级包...\")\n",
        "!pip install -q fire tqdm PyYAML packaging\n",
        "\n",
        "# 尝试安装transformers（如果需要升级）\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"✅ 使用现有Transformers: {transformers.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"📥 安装Transformers...\")\n",
        "    !pip install -q transformers\n",
        "\n",
        "# 尝试安装MONAI（如果需要）\n",
        "try:\n",
        "    import monai\n",
        "    print(f\"✅ 使用现有MONAI: {monai.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"📥 安装MONAI...\")\n",
        "    !pip install -q monai\n",
        "\n",
        "print(\"✅ 极简安装完成！\")\n",
        "print(\"🎯 策略：使用Colab现有环境，最小化安装\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📋 验证安装结果\n",
        "\n",
        "运行下面的cell检查安装是否成功：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 简单的安装验证（不检查冲突）\n",
        "print(\"🔍 验证核心包是否可用...\")\n",
        "\n",
        "# 检查PyTorch\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"✅ PyTorch: {torch.__version__} (CUDA: {torch.cuda.is_available()})\")\n",
        "except ImportError:\n",
        "    print(\"❌ PyTorch不可用\")\n",
        "\n",
        "# 检查Transformers  \n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"✅ Transformers: {transformers.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"❌ Transformers不可用\")\n",
        "\n",
        "# 检查MONAI\n",
        "try:\n",
        "    import monai\n",
        "    print(f\"✅ MONAI: {monai.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"❌ MONAI不可用\")\n",
        "\n",
        "# 检查其他必需包\n",
        "essential_packages = ['fire', 'tqdm', 'yaml', 'packaging']\n",
        "for pkg in essential_packages:\n",
        "    try:\n",
        "        if pkg == 'yaml':\n",
        "            import yaml\n",
        "        else:\n",
        "            __import__(pkg)\n",
        "        print(f\"✅ {pkg} 可用\")\n",
        "    except ImportError:\n",
        "        print(f\"❌ {pkg} 不可用\")\n",
        "\n",
        "print(\"\\n🎯 如果核心包都可用，就可以开始训练了！\")\n",
        "print(\"💡 忽略pip依赖冲突警告 - 它们通常不影响实际功能\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MONAI专用修复 (如果MONAI导入失败)\n",
        "\n",
        "如果遇到 `'FileFinder' object has no attribute 'find_module'` 错误，运行下面的cell：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MONAI Python 3.12 兼容性修复\n",
        "print(\"🔧 修复MONAI Python 3.12兼容性问题...\")\n",
        "\n",
        "# Uninstall and reinstall MONAI with specific fixes\n",
        "!pip uninstall -y monai\n",
        "!pip install --no-cache-dir --force-reinstall monai[all]==1.3.0\n",
        "\n",
        "# Alternative: Install from source if needed\n",
        "# !pip install git+https://github.com/Project-MONAI/MONAI.git\n",
        "\n",
        "print(\"✅ MONAI修复完成！请重启runtime并重新验证。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone Repository and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "repo_url = \"https://github.com/tanglehunter00/AR-SSL4M-DEMO.git\"\n",
        "\n",
        "if os.path.exists(\"AR-SSL4M-DEMO\"):\n",
        "    print(\"Repository already exists, pulling latest changes...\")\n",
        "    !cd AR-SSL4M-DEMO && git pull\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone {repo_url}\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(\"AR-SSL4M-DEMO\")\n",
        "print(f\"✅ Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Setup\n",
        "\n",
        "根据您的截图，数据集位于Google Drive的 `dataset/compressed_datasets/volumes` 路径中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive to access your data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to your data in Google Drive (based on your screenshot)\n",
        "data_path = \"/content/drive/MyDrive/dataset/compressed_datasets/volumes\"\n",
        "\n",
        "# Check if data exists\n",
        "if os.path.exists(data_path):\n",
        "    npy_files = [f for f in os.listdir(data_path) if f.endswith('.npy')]\n",
        "    print(f\"✅ Found {len(npy_files)} .npy files in {data_path}\")\n",
        "    if len(npy_files) == 0:\n",
        "        print(\"⚠️ No .npy files found in the directory\")\n",
        "        print(\"Available files:\")\n",
        "        all_files = os.listdir(data_path)[:10]  # Show first 10 files\n",
        "        for f in all_files:\n",
        "            print(f\"  - {f}\")\n",
        "else:\n",
        "    print(f\"❌ Data path not found: {data_path}\")\n",
        "    print(\"Available paths in Google Drive:\")\n",
        "    try:\n",
        "        drive_contents = os.listdir(\"/content/drive/MyDrive\")\n",
        "        for item in drive_contents[:10]:\n",
        "            print(f\"  - /content/drive/MyDrive/{item}\")\n",
        "    except:\n",
        "        print(\"Unable to list drive contents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data list file\n",
        "import glob\n",
        "\n",
        "# Try multiple possible data paths\n",
        "possible_paths = [\n",
        "    \"/content/drive/MyDrive/dataset/compressed_datasets/volumes\",\n",
        "    \"/content/drive/MyDrive/compressed_datasets/volumes\", \n",
        "    \"/content/drive/MyDrive/dataset/volumes\",\n",
        "    \"/content/drive/MyDrive/volumes\"\n",
        "]\n",
        "\n",
        "npy_files = []\n",
        "actual_data_path = None\n",
        "\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        files = glob.glob(os.path.join(path, \"*.npy\"))\n",
        "        if files:\n",
        "            npy_files = files\n",
        "            actual_data_path = path\n",
        "            print(f\"✅ Found {len(npy_files)} .npy files in {actual_data_path}\")\n",
        "            break\n",
        "        else:\n",
        "            print(f\"⚠️ Path exists but no .npy files found: {path}\")\n",
        "\n",
        "if not npy_files:\n",
        "    print(\"❌ No .npy files found in any of the expected paths\")\n",
        "    print(\"Please check your Google Drive structure and update the paths accordingly\")\n",
        "else:\n",
        "    # Create data list file\n",
        "    data_list_path = \"pretrain/colab_data_list.txt\"\n",
        "    with open(data_list_path, 'w') as f:\n",
        "        for npy_file in npy_files:\n",
        "            f.write(f\"{npy_file}\\n\")\n",
        "    \n",
        "    print(f\"✅ Created data list: {data_list_path} with {len(npy_files)} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Update Configuration for Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update dataset configuration\n",
        "config_content = '''from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class custom_dataset:\n",
        "    dataset: str = \"custom_dataset\"\n",
        "    file: str = \"image_dataset.py\"\n",
        "    train_split: str = \"train\"\n",
        "    test_split: str = \"validation\"\n",
        "    spatial_path: str = \"colab_data_list.txt\"\n",
        "    contrast_path: str = \"colab_data_list.txt\"\n",
        "    semantic_path: str = \"colab_data_list.txt\"\n",
        "    img_size = [128, 128, 128]\n",
        "    patch_size = [16, 16, 16]\n",
        "    attention_type = 'prefix'\n",
        "    add_series_data = False\n",
        "    add_spatial_data = True\n",
        "    is_subset = False\n",
        "    series_length = 4\n",
        "'''\n",
        "\n",
        "with open('pretrain/configs/datasets.py', 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"✅ Updated dataset configuration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update training configuration for Colab\n",
        "training_config_content = '''from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class train_config:\n",
        "    enable_fsdp: bool=False\n",
        "    low_cpu_fsdp: bool=False\n",
        "    run_validation: bool=True\n",
        "    batch_size_training: int=4  # Adjusted for Colab GPU memory\n",
        "    batching_strategy: str=\"padding\"\n",
        "    gradient_accumulation_steps: int=1\n",
        "    gradient_clipping: bool=False\n",
        "    gradient_clipping_threshold: float = 1.0\n",
        "    num_epochs: int=1  # Single epoch for Colab\n",
        "    warmup_epochs:int=0\n",
        "    num_workers_dataloader: int=0  # Set to 0 to avoid multiprocessing issues in Colab\n",
        "    lr: float=1e-4\n",
        "    weight_decay: float=0.01\n",
        "    gamma: float=0.1\n",
        "    seed: int=42\n",
        "    use_fp16: bool=True  # Enable FP16 for Colab GPU memory efficiency\n",
        "    mixed_precision: bool=True\n",
        "    val_batch_size: int=1\n",
        "    dataset = \"custom_dataset\"\n",
        "    output_dir: str=\"/content/AR-SSL4M-DEMO/pretrain/save\"\n",
        "    freeze_layers: bool=False\n",
        "    num_freeze_layers: int=1\n",
        "    save_model: bool=True\n",
        "    save_optimizer: bool=False\n",
        "    save_metrics: bool=True\n",
        "    scheduler:str='CosineLR'\n",
        "    min_lr: float=0\n",
        "    pos_type: str='sincos3d'\n",
        "    norm_pixel_loss: bool=True\n",
        "    enable_profiling: bool=False  # Disable profiling for cleaner output\n",
        "'''\n",
        "\n",
        "with open('pretrain/configs/training.py', 'w') as f:\n",
        "    f.write(training_config_content)\n",
        "\n",
        "print(\"✅ Updated training configuration for Colab\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Start Pretraining\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create save directory\n",
        "os.makedirs(\"pretrain/save\", exist_ok=True)\n",
        "print(\"✅ Created save directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start pretraining\n",
        "print(\"🚀 Starting AR-SSL4M Pretraining...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Change to pretrain directory and run\n",
        "os.chdir(\"pretrain\")\n",
        "\n",
        "# Run the training script\n",
        "!python main.py --output_dir save --batch_size_training 4\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"✅ Pretraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Check Results and Download Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training results\n",
        "import os\n",
        "import glob\n",
        "\n",
        "save_dir = \"save\"\n",
        "if os.path.exists(save_dir):\n",
        "    files = os.listdir(save_dir)\n",
        "    print(f\"📁 Files in save directory:\")\n",
        "    for file in files:\n",
        "        file_path = os.path.join(save_dir, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "            print(f\"  - {file} ({size_mb:.1f} MB)\")\n",
        "else:\n",
        "    print(\"❌ Save directory not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the trained model\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# Create a zip file with all results\n",
        "zip_path = \"/content/ar_ssl4m_pretrained_model.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    for root, dirs, files_list in os.walk(\"save\"):\n",
        "        for file in files_list:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, \"save\")\n",
        "            zipf.write(file_path, arcname)\n",
        "            print(f\"Added to zip: {arcname}\")\n",
        "\n",
        "print(f\"\\n📦 Created zip file: {zip_path}\")\n",
        "print(\"⬇️ Downloading...\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training summary\n",
        "print(\"🎯 AR-SSL4M Pretraining Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"📊 Dataset: STOIC ({len(npy_files) if 'npy_files' in locals() and npy_files else 'Unknown'} samples)\")\n",
        "print(f\"📂 Data path: {actual_data_path if 'actual_data_path' in locals() and actual_data_path else 'Not found'}\")\n",
        "print(f\"🏗️ Model: AR-SSL4M (91.3M parameters)\")\n",
        "print(f\"📐 Image size: 128×128×128\")\n",
        "print(f\"🔢 Patch size: 16×16×16\")\n",
        "print(f\"📦 Batch size: 4\")\n",
        "print(f\"🎓 Learning rate: 1e-4\")\n",
        "print(f\"🔄 Epochs: 1\")\n",
        "print(f\"💾 Results saved to: save/\")\n",
        "print(f\"🔗 GitHub Repo: https://github.com/tanglehunter00/AR-SSL4M-DEMO\")\n",
        "print(\"=\" * 50)\n",
        "print(\"✅ Training completed successfully!\")\n",
        "print(\"\\n📝 Next steps:\")\n",
        "print(\"1. Download the model zip file\")\n",
        "print(\"2. Use the pretrained model for downstream tasks\")\n",
        "print(\"3. Fine-tune on specific medical imaging tasks (segmentation, classification, etc.)\")\n",
        "print(\"4. Experiment with different hyperparameters for better performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/tanglehunter00/AR-SSL4M-DEMO.git"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
