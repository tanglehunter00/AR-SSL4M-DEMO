{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 挂载 Google Drive (用于读取存储在 Drive 上的 LIDC-IDRI 原始数据集)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. 从 GitHub 克隆项目代码\n",
    "# 注意：确保您已经将之前创建的 preprocess/proc_lidc_idri.py 推送到了该仓库\n",
    "!git clone https://github.com/tanglehunter00/AR-SSL4M-DEMO.git\n",
    "\n",
    "# 3. 设置项目路径并切换工作目录\n",
    "project_root = '/content/AR-SSL4M-DEMO' \n",
    "import os\n",
    "if os.path.exists(project_root):\n",
    "    %cd {project_root}\n",
    "    print(f\"已进入项目根目录: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"项目克隆失败，请检查仓库地址或网络连接。\")\n",
    "\n",
    "# 4. 安装预处理所需的额外依赖\n",
    "!pip install pydicom SimpleITK monai\n",
    "\n",
    "# 5. 定义数据集在 Drive 上的路径\n",
    "# 根据您的结构：My Drive > dataset > LIDC-IDRI > LIDC-IDRI\n",
    "dataset_path = \"/content/drive/MyDrive/dataset/LIDC-IDRI/LIDC-IDRI\"\n",
    "\n",
    "# 6. 执行预处理脚本\n",
    "# 我们将处理结果保存在项目内的 pretrain/data/patch_random_lidc 目录下\n",
    "!python preprocess/proc_lidc_idri.py \"{dataset_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97549189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 挂载 Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. 安装必要依赖\n",
    "!pip install monai nibabel\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from monai.transforms import Compose, Resize, ScaleIntensityRangePercentiles\n",
    "\n",
    "# --- 核心处理逻辑 ---\n",
    "\n",
    "def load_nii_data(filename):\n",
    "    import nibabel as nib\n",
    "    img = nib.load(filename)\n",
    "    return img.get_fdata(), img.affine\n",
    "\n",
    "def cut_patch(image, patch_size):\n",
    "    z, y, x = image.shape\n",
    "    pz, py, px = patch_size\n",
    "    z1 = random.randint(0, z - pz)\n",
    "    y1 = random.randint(0, y - py)\n",
    "    x1 = random.randint(0, x - px)\n",
    "    return image[z1:z1+pz, y1:y1+py, x1:x1+px], (z1, z1+pz, y1, y1+py, x1, x1+px)\n",
    "\n",
    "def load_and_patch_transforms_series(img, tar_img_size):\n",
    "    transforms = Compose([\n",
    "        Resize(spatial_size=(tar_img_size[0], tar_img_size[1], tar_img_size[2]), mode='trilinear'),\n",
    "        ScaleIntensityRangePercentiles(lower=1., upper=99.9, b_min=0.0, b_max=1.0, clip=True, relative=False, channel_wise=False),\n",
    "    ])\n",
    "    return transforms(img)\n",
    "\n",
    "def process_one_case(image_file, patch_size_list, patch_num, save_root, tar_img_size):\n",
    "    # 本地临时存放点 (Colab 磁盘，非 Drive)\n",
    "    local_tmp = \"/content/temp_case\"\n",
    "    if os.path.exists(local_tmp):\n",
    "        shutil.rmtree(local_tmp)\n",
    "    os.makedirs(local_tmp)\n",
    "\n",
    "    try:\n",
    "        # 识别后缀\n",
    "        ext = \".nii.gz\" if image_file.endswith(\".nii.gz\") else \".nii\"\n",
    "\n",
    "        # 解析 ID 用于命名\n",
    "        path_parts = image_file.split('/')\n",
    "        ds_name = path_parts[-3] if len(path_parts) > 3 else \"BraTS\"\n",
    "        nii_id = os.path.basename(image_file).replace(f'-t1n{ext}', '')\n",
    "\n",
    "        # 目标压缩包路径\n",
    "        tar_file_path = os.path.join(save_root, f\"{ds_name}_{nii_id}.tar.gz\")\n",
    "\n",
    "        # 检查是否已存在，实现断点续传\n",
    "        if os.path.exists(tar_file_path):\n",
    "            print(f\"⏩ 跳过已存在的病例: {nii_id}\")\n",
    "            return True\n",
    "\n",
    "        # 加载 4 个模态\n",
    "        image, affine = load_nii_data(image_file)\n",
    "        image_s1, _ = load_nii_data(image_file.replace(f't1n{ext}', f't1c{ext}'))\n",
    "        image_s2, _ = load_nii_data(image_file.replace(f't1n{ext}', f't2w{ext}'))\n",
    "        image_s3, _ = load_nii_data(image_file.replace(f't1n{ext}', f't2f{ext}'))\n",
    "\n",
    "        # 转换坐标轴 -> z,y,x\n",
    "        image = image.transpose((2, 1, 0))\n",
    "        image_s1 = image_s1.transpose((2, 1, 0))\n",
    "        image_s2 = image_s2.transpose((2, 1, 0))\n",
    "        image_s3 = image_s3.transpose((2, 1, 0))\n",
    "\n",
    "        # 1. 生成 400 个 npy 文件到本地临时目录\n",
    "        for i in range(patch_num):\n",
    "            patch_size = random.choice(patch_size_list)\n",
    "            image_patch, cut_size = cut_patch(image, patch_size)\n",
    "            z1, z2, y1, y2, x1, x2 = cut_size\n",
    "\n",
    "            # 切块并转换\n",
    "            p0 = load_and_patch_transforms_series(np.expand_dims(image_patch.transpose((2, 1, 0)), 0), tar_img_size).numpy()[0]\n",
    "            p1 = load_and_patch_transforms_series(np.expand_dims(image_s1[z1:z2, y1:y2, x1:x2].transpose((2, 1, 0)), 0), tar_img_size).numpy()[0]\n",
    "            p2 = load_and_patch_transforms_series(np.expand_dims(image_s2[z1:z2, y1:y2, x1:x2].transpose((2, 1, 0)), 0), tar_img_size).numpy()[0]\n",
    "            p3 = load_and_patch_transforms_series(np.expand_dims(image_s3[z1:z2, y1:y2, x1:x2].transpose((2, 1, 0)), 0), tar_img_size).numpy()[0]\n",
    "\n",
    "            base_name = f\"{ds_name}_{nii_id}_{i}\"\n",
    "            np.save(os.path.join(local_tmp, f\"{base_name}.t1n.npy\"), p0)\n",
    "            np.save(os.path.join(local_tmp, f\"{base_name}.t1c.npy\"), p1)\n",
    "            np.save(os.path.join(local_tmp, f\"{base_name}.t2w.npy\"), p2)\n",
    "            np.save(os.path.join(local_tmp, f\"{base_name}.t2f.npy\"), p3)\n",
    "\n",
    "        # 2. 将这 400 个文件打包成一个 tar.gz\n",
    "        with tarfile.open(tar_file_path, \"w:gz\") as tar:\n",
    "            tar.add(local_tmp, arcname=nii_id) # 打包并重命名内部文件夹\n",
    "\n",
    "        # 3. 清理本地临时文件\n",
    "        shutil.rmtree(local_tmp)\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {image_file}: {e}\")\n",
    "        if os.path.exists(local_tmp):\n",
    "            shutil.rmtree(local_tmp)\n",
    "        return False\n",
    "\n",
    "# --- Colab 运行主逻辑 ---\n",
    "\n",
    "# 1. 设置输入根目录\n",
    "INPUT_ROOT = \"/content/drive/MyDrive/dataset/pretrain/BraTS23_Data/Data/BraTS-GLI/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/A-GLI-Part-03\"\n",
    "\n",
    "# 2. 设置输出目录 (建议新开一个目录存放 tar.gz)\n",
    "SAVE_ROOT = \"/content/drive/MyDrive/dataset/pretrain/BraTS23_Data/tar_data\"\n",
    "if not os.path.exists(SAVE_ROOT):\n",
    "    os.makedirs(SAVE_ROOT)\n",
    "\n",
    "# 3. 参数设置\n",
    "patch_num = 100\n",
    "tar_img_size = [128, 128, 32]\n",
    "patch_size_list = [(32, 128, 128)]\n",
    "\n",
    "# 4. 扫描所有病例\n",
    "print(f\"正在扫描目录: {INPUT_ROOT} ...\")\n",
    "all_t1n_files = []\n",
    "for root, dirs, files in os.walk(INPUT_ROOT):\n",
    "    for f in files:\n",
    "        if f.endswith(\"t1n.nii.gz\") or f.endswith(\"t1n.nii\"):\n",
    "            all_t1n_files.append(os.path.join(root, f))\n",
    "\n",
    "all_t1n_files.sort()\n",
    "print(f\"找到 {len(all_t1n_files)} 个病例。\")\n",
    "\n",
    "# 5. 开始处理\n",
    "start_time = time.time()\n",
    "success_count = 0\n",
    "\n",
    "for i, file_path in enumerate(all_t1n_files):\n",
    "    print(f\"[{i+1}/{len(all_t1n_files)}] 正在处理并打包: {os.path.basename(file_path)}\")\n",
    "    if process_one_case(file_path, patch_size_list, patch_num, SAVE_ROOT, tar_img_size):\n",
    "        success_count += 1\n",
    "\n",
    "    # 每 5 个病例同步一次磁盘，确保安全\n",
    "    if (i+1) % 5 == 0:\n",
    "        os.sync()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n✨ 处理完成！\")\n",
    "print(f\"成功打包: {success_count} 个病例\")\n",
    "print(f\"总耗时: {(end_time - start_time)/60:.2f} 分钟\")\n",
    "print(f\"所有压缩包已存至: {SAVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c614f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 安装必要的库 (MONAI 是原代码处理的核心)\n",
    "!pip install monai nibabel\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "from google.colab import drive\n",
    "from monai.transforms import Compose, Resize, ScaleIntensityRangePercentiles\n",
    "\n",
    "# 2. 挂载 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 3. 定义路径（根据你的要求设置）\n",
    "BASE_PATH = '/content/drive/MyDrive/dataset/pretrain/DeepLesion/data'\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'Images_nifti')\n",
    "ANNO_PATH = os.path.join(BASE_PATH, 'DL_info.csv')\n",
    "SAVE_ROOT = os.path.join(BASE_PATH, 'npy')\n",
    "\n",
    "# 创建输出目录\n",
    "if not os.path.exists(SAVE_ROOT):\n",
    "    os.makedirs(SAVE_ROOT)\n",
    "    print(f\"创建文件夹: {SAVE_ROOT}\")\n",
    "\n",
    "# 4. 核心处理函数 (完全遵循原项目逻辑)\n",
    "def readCSV(filename):\n",
    "    lines = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        for line in csvreader:\n",
    "            lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def load_and_patch_transforms(img, tar_img_size):\n",
    "    \"\"\"遵循原代码的变换逻辑\"\"\"\n",
    "    transforms = Compose([\n",
    "        Resize(spatial_size=tar_img_size, mode='trilinear'),\n",
    "        ScaleIntensityRangePercentiles(\n",
    "            lower=1., upper=99.9, \n",
    "            b_min=0.0, b_max=1.0, \n",
    "            clip=True, relative=False, channel_wise=False\n",
    "        ),\n",
    "    ])\n",
    "    return transforms(img)\n",
    "\n",
    "def process_deeplesion():\n",
    "    tar_img_size = [128, 128, 32]\n",
    "    annos = readCSV(ANNO_PATH)\n",
    "    \n",
    "    print(f\"开始处理，总标注行数: {len(annos)-1}\")\n",
    "    \n",
    "    # 从索引 1 开始（跳过表头）\n",
    "    for index in range(1, len(annos)):\n",
    "        anno = annos[index]\n",
    "        \n",
    "        # 准则 1: 必须有粗粒度标签 (anno[9] != '-1')\n",
    "        if anno[9] == '-1':\n",
    "            continue\n",
    "\n",
    "        # 解析信息\n",
    "        image_name = '_'.join(anno[0].split('_')[:3])\n",
    "        # 获取 Bounding Box 中心点\n",
    "        bbox = [float(x.replace('[', '').replace(']', '').strip()) for x in anno[6].split(',')]\n",
    "        x_center, y_center = int((bbox[0] + bbox[2]) // 2), int((bbox[1] + bbox[3]) // 2)\n",
    "        # 获取切片范围用于匹配文件\n",
    "        slice_range = [anno[11].split(',')[0], anno[11].split(',')[-1].split(' ')[-1].strip()]\n",
    "\n",
    "        # 查找匹配的 NIfTI 文件\n",
    "        search_pattern = os.path.join(DATA_PATH, f'{image_name}_*{slice_range[0]}-*{slice_range[1]}.nii.gz')\n",
    "        match_files = glob(search_pattern)\n",
    "        \n",
    "        if not match_files:\n",
    "            # print(f\"跳过: 未找到匹配文件 {search_pattern}\")\n",
    "            continue\n",
    "\n",
    "        nii_name = match_files[0]\n",
    "        \n",
    "        try:\n",
    "            # 加载数据\n",
    "            nii_img = nib.load(nii_name)\n",
    "            nii_data = nii_img.get_fdata()\n",
    "            \n",
    "            x_shape, y_shape = nii_data.shape[0], nii_data.shape[1]\n",
    "            \n",
    "            # 准则 2: 以病灶中心裁剪 128x128 的 Patch\n",
    "            # 注意：原代码逻辑中使用 y_center 对应 shape[0], x_center 对应 shape[1]\n",
    "            image_patch = nii_data[\n",
    "                max(0, y_center - 64): min(y_center + 64, x_shape), \n",
    "                max(0, x_center - 64): min(x_center + 64, y_shape), \n",
    "                :\n",
    "            ]\n",
    "            \n",
    "            # 准则 3: Resize 128x128x32 & 强度归一化\n",
    "            # 增加 Channel 维度满足 MONAI 要求 [C, H, W, D]\n",
    "            image_patch_input = np.expand_dims(image_patch, 0)\n",
    "            processed_patch = load_and_patch_transforms(image_patch_input, tar_img_size)\n",
    "            \n",
    "            # 移除 Channel 维度还原为 [H, W, D]\n",
    "            final_data = processed_patch.numpy()[0, ...]\n",
    "\n",
    "            # 准则 4: 保存为 .npy (一病灶一文件)\n",
    "            # 命名规则: {image_name}_{行索引}_{标签}.npy\n",
    "            save_name = os.path.join(SAVE_ROOT, f'{image_name}_{index}_{anno[9]}.npy')\n",
    "            np.save(save_name, final_data)\n",
    "            \n",
    "            if index % 100 == 0:\n",
    "                print(f\"已处理 {index} 条记录...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"错误发生于索引 {index}: {str(e)}\")\n",
    "\n",
    "    print(\"处理完成！所有文件已保存至 Google Drive。\")\n",
    "\n",
    "# 执行处理\n",
    "process_deeplesion()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
